{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1efe2220",
   "metadata": {},
   "source": [
    "# Progress of the Philippines' Sustainable Development Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5cba3",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ef5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db140c",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "The following **csv** files used in this project are acquired through a request sent to the Knowledge Management and Communications Division of the Philippine Statistics Authority.\n",
    "\n",
    "### Combining the Datasets \n",
    "In this stage, the separate datasets underwent pre-processing and cleaning before they are combined together. Some of the cleaning done on each of the datasets are: (1) fixing of column names, (2) modification of the values of the 'Geolocation' column, (3) removal of unneeded rows and columns, and (4) conversion of '..' or '...' values to NaN. After this, the dataset is converted into a long representation before they are merged together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39340350",
   "metadata": {},
   "source": [
    "#### 1.2.1. Proportion of population living below the national poverty line \n",
    "To start with, let us load the data from the csv file using pandas' [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function.\n",
    "\n",
    "The [`os.getenv`](https://docs.python.org/3/library/os.html) function was used to get the environment variable `DSDATA_PROJ`, which points to the data folder of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14568c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/1.2.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/1.2.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b658bd",
   "metadata": {},
   "source": [
    "Looking at the DataFrame, we could see that the columns are unnamed and that the column names are located at the 0th row. Using [`iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html), we could get the 0th row and then assign it as the column values. \n",
    "\n",
    "Then, using the [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) function, we can drop the 0th row as we have no need for it anymore. Additionally, since the row at index 1 is a row full of NaN, we can also drop it using the same function. \n",
    "\n",
    "To be able to fix the indexing of the rows, the [`reset_index`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) function was used to reset the index from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae001ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting our column names\n",
    "data.columns = data.iloc [0] \n",
    "\n",
    "# dropping the 'geolocation' row as that is actually used as a header\n",
    "data = data.drop (data.index [1])\n",
    "\n",
    "# dropping the column names \n",
    "data = data.drop (data.index [0])\n",
    "\n",
    "data.reset_index (drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79485cc1",
   "metadata": {},
   "source": [
    "Irrelevant rows that are just footers for the file are also removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8bc10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant rows \n",
    "data = data.drop (data.index [18:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebebaa06",
   "metadata": {},
   "source": [
    "The `Year` column must also be renamed into `Geolocation` as this row refers to the different regions in the Philippines, and not the years. This can be done through the use of the of the [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5184c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the column 'Year' as its actually the location column\n",
    "data.rename(columns = {'Year':'Geolocation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d0c896",
   "metadata": {},
   "source": [
    "To easily determine which region the `Geolocation` values refer to, we can also change these values to include the names that they are commonly referred to, instead of just their region numbers. \n",
    "\n",
    "For consistency throughout the different datasets, the `region_names` variable was declared. The reason why a map was not used was that different datasets have different representations of the region (i.e., differences in naming a region), however, they are always arranged in the same way. This would be shown below in the pre-processing of each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Before applying, make sure that the arrangement of the regions are the same as the arrangement in your table\n",
    "region_names = ['PHILIPPINES', 'NCR: National Capital Region', \n",
    "                 'CAR: Cordillera Administrative Region', \n",
    "                 'Region 1: Ilocos Region', \n",
    "                 'Region 2: Cagayan Valley', \n",
    "                 'Region 3: Central Luzon', \n",
    "                 'Region 4A: CALABARZON', \n",
    "                'MIMAROPA: Southwestern Tagalog Region', \n",
    "                'Region 5: Bicol Region', \n",
    "                'Region 6: Western Visayas', \n",
    "                'Region 7: Central Visayas', \n",
    "                'Region 8: Eastern Visayas', \n",
    "                'Region 9: Zamboanga Peninsula', \n",
    "                'Region 10: Northern Mindanao', \n",
    "                'Region 11: Davao Region', \n",
    "                'Region 12: SOCCSKSARGEN', \n",
    "                'CARAGA: Cordillera Administrative Region', \n",
    "                'BARMM: Bangsamoro Autonomous Region in Muslim Mindanao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95da29a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Geolocation'] = region_names\n",
    "data.set_index('Geolocation')\n",
    "data = data.reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2153b",
   "metadata": {},
   "source": [
    "Next, we can convert the strings of '..' and '...', which were used to represent that there were no values for these cells, to **NaN**, through the use of the [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) function.\n",
    "\n",
    "However, the columns that have all **NaN** values were not dropped because if this dataset would be combined with other datasets, all years would still be present as there are datasets with complete data for all the years. Additionally, dropping the years for some of the dataset would result in the combined dataset having a weird sorting (i.e., a sorting of the region that does not follow the usual sorting of the datasets in the Philippines), even if it was sorted based on the `Year` and `Geolocation` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce63be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    # cells without values are represented as either '..' or '...', so we should convert them to NaN so we could dropna()\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# drops columns if all of the values are NaN\n",
    "# data = data.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971705fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c523309",
   "metadata": {},
   "source": [
    "As the final step, the wide representation of this dataset is converted to a long representation through the use of the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function. \n",
    "\n",
    "Then, the column that holds the value for a specific year and region is coverted, using [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html), to the ID of this Sustainable Development Goal (SDG), so that it can be distinguished when it is combined with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372cde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'1.2.1. Proportion of population living below the national poverty line', 0 : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20284161",
   "metadata": {},
   "source": [
    "As this is the first dataset, we can just assign it to the `combined_data` DataFrame, which would hold the combined datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798efeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca9864",
   "metadata": {},
   "source": [
    "#### 1.4.1p5. Net Enrolment Rate in elementary\n",
    "\n",
    "Using the same [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function, we load the next dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f265d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/1.4.1p5.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/1.4.1p5.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd020ae5",
   "metadata": {},
   "source": [
    "From the DataFrame above, we can see that the footer of the .csv files was included in the DataFrame. As the rows from the 56th index are irrelevant, we can [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (data.index [56:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce36258",
   "metadata": {},
   "source": [
    "Additionally, we can see that the columns are unnamed, and upon inspection, the original column names can be found at `Index 0`. Thus, we can set the columns to this row, and then  [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the `Index 0` row as it would only be redundant and might affect the computations.\n",
    "\n",
    "The [`reset_index`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) function was used in order to make the index of the rows start from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cddbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the column names and removing the row that held the previous column names\n",
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9007900e",
   "metadata": {},
   "source": [
    "However, these is still a row of NaN found at `Index 0`, and we can see that the column names for the first two columns are not correct for the values underneath it, as the ones under the first column are actually Geolocations and those under the second columns are the values for Sex. Thus, we can [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) it, and then [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)  the row at `Index 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231defb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns = {np.nan:'Geolocation', 'Year': 'Sex'})\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d6fbf",
   "metadata": {},
   "source": [
    "As we would only need the data that is grouped by region and not by sex, we would only be getting the rows that has **Both Sexes** as the value in the Sex column. After this, we can [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the Sex column as it would not be used onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c99d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only getting the total data, then dropping Sex column as it's not needed anymore\n",
    "data = data[data['Sex'] == 'Both Sexes']\n",
    "data = data.drop(\"Sex\", axis = 1)\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5019ce",
   "metadata": {},
   "source": [
    "To be able to merge this to the combined DataFrame, the value of the Geolocation column has been set to the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdfb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1217f7e",
   "metadata": {},
   "source": [
    "Since the dataset represents missing values as either '...' or '..', we can [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) the columns with these values with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42240c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    # cells without values are represented as either '..' or '...', so we should convert them to NaN so we could dropna()\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53f166",
   "metadata": {},
   "source": [
    "Then, we can transform the wide representation of the DataFrame to its long representation version using the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'1.4.1p5 Net Enrolment Rate in elementary (Indicator is also found in SDG 4.3.s1) 1/', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd4ddcc",
   "metadata": {},
   "source": [
    "Then we can [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) this long representation DataFrame into the combined DataFrame. It would be merged with respect to the values in the **Geolocation** and **Year** column. An outer join is used as we want to retain all the values of both of the DataFrames, even if there would be **NaN** values for some of cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b615c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c476809",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9171bad",
   "metadata": {},
   "source": [
    "#### 1.4.1p6. Net Enrolment Rate in secondary education (Indicator is also found in SDG 4.3.s2)\n",
    "\n",
    "Next, we can load the third dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc03bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/1.4.1p6.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/1.4.1p6.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c4063",
   "metadata": {},
   "source": [
    "Just like in the processing of the previous datasets, we first [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the unnecessary rows at the bottom part of the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161badf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (data.index [110:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc3c24",
   "metadata": {},
   "source": [
    "From the DataFrame above, we can see that the correct column headers are found at `Index 0`. However, upon inspection, we would see that there are two NaN values and the 'Year' value at the third column should actually be 'Sex' based on the values below it. Thus, before setting this row as the column header, we first correct the values of these first three columns using the [`at`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0, '1.4.1p6 Net Enrolment Rate in secondary education (Indicator is also found in SDG 4.3.s2)'] = 'Level of Education'\n",
    "data.at[0, 'Unnamed: 1'] = 'Geolocation'\n",
    "data.at[0, 'Unnamed: 2'] = 'Sex'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c18347",
   "metadata": {},
   "source": [
    "Now that first row can correctly act as the column header, we can set is as the column header, before dropping the row at `Index 0`. Then we must also [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the row of **NaN**s at `Index 1` as it is unnecessary, before using the [`reset_index`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd410f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0876e",
   "metadata": {},
   "source": [
    "Using the [`unique`](https://pandas.pydata.org/docs/reference/api/pandas.unique.html) function, we can see that there are two values for 'Level of Education' columns. To be able to combine this to the combined dataset, we must separate them as we cannot add another column that would hold the education level, thus, we can just add it as two different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Level of Education'].unique ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96be64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_data = data [54:]\n",
    "junior_high_data = data [:54]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c98c9b5",
   "metadata": {},
   "source": [
    "Now, we must process these two separately, but the processes done to them would be the same.\n",
    "\n",
    "First, as we only need the general data, without taking *Sex* into consideration. This can be done by only getting the rows that has **Both Sexes** as the value of the `Sex` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_high_data = junior_high_data [junior_high_data['Sex'] == 'Both Sexes']\n",
    "junior_high_data = junior_high_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_data = senior_high_data [senior_high_data['Sex'] == 'Both Sexes']\n",
    "senior_high_data = senior_high_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc1934-b819-4802-a81e-e3c6d4f3483c",
   "metadata": {},
   "source": [
    "Next, as we have already separated the dataset into two based on the value of the `Level of Education` column, we have no need for this column anymore. This means that we can [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) this column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f57eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_high_data = junior_high_data.drop(\"Level of Education\", axis = 1)\n",
    "junior_high_data = junior_high_data.drop(\"Sex\", axis = 1)\n",
    "junior_high_data = junior_high_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a100777",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_data = senior_high_data.drop(\"Level of Education\", axis = 1)\n",
    "senior_high_data = senior_high_data.drop(\"Sex\", axis = 1)\n",
    "senior_high_data = senior_high_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d6d32-8c2b-4bf3-925a-b9281614a412",
   "metadata": {},
   "source": [
    "For consistency, we set the values of the `Geolocation` column to the format of the region names that we have decided before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_data['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_high_data['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d82bf3-74bd-47a4-a2a1-01acc440200e",
   "metadata": {},
   "source": [
    "As the dataset represents missing values as '..' or '...', we must [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.Series.replace.html) these values with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in junior_high_data.columns.difference(['Geolocation']):\n",
    "    junior_high_data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    junior_high_data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b4c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in senior_high_data.columns.difference(['Geolocation']):\n",
    "    senior_high_data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    senior_high_data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e0ccd1-fe39-4114-8565-c04122ef45c4",
   "metadata": {},
   "source": [
    "Looking at the senior high data, we can see that all of the values are `NaN` from 2000 to 2016, which is to be expected as Senior High School was only implemented from 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2e75e-2cb5-41fe-b2e0-04fb9cc251ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f67ba3-7375-48c7-afe8-a689a5b4c1f9",
   "metadata": {},
   "source": [
    "Next, we can convert both of the datasets into its long representation using the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33932bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_high_data = pd.melt(junior_high_data, id_vars='Geolocation', value_vars=junior_high_data.columns [1:]) \n",
    "\n",
    "junior_high_data.rename(columns = {'value':'1.4.1p6 Net Enrolment Rate in secondary education (Junior High School)', 0 : 'Year'}, inplace=True)\n",
    "junior_high_data = junior_high_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c5df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_data = pd.melt(senior_high_data, id_vars='Geolocation', value_vars=senior_high_data.columns [1:]) \n",
    "\n",
    "senior_high_data.rename(columns = {'value':'1.4.1p6 Net Enrolment Rate in secondary education (Senior High School)', 0 : 'Year'}, inplace=True)\n",
    "senior_high_data = senior_high_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f187a10-c3c1-4f9c-b984-d4e2b1b64a2e",
   "metadata": {},
   "source": [
    "Once that both datasets has been converted to their long representation, we can [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) the two datasets to the combined dataset based on the values of the `Geolocation` and the `Year` column with an outer join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(junior_high_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.merge(senior_high_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ed15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c643a5d",
   "metadata": {},
   "source": [
    "#### 1.5.4. Proportion of local governments that adopt and implement local disaster risk reduction strategies in line with national disaster risk reduction strategies\n",
    "Then, the fourth dataset could be loaded using the same [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58698f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/1.5.4.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/1.5.4.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca9665-aecc-4cba-a237-662f2602e81e",
   "metadata": {},
   "source": [
    "Same as the previous datasets, we would need to [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the irrelevant rows at the bottom of the DataFrame. These are the rows that were a footer outside of the table in the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190f147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop (data.index [19:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e7501-60e7-4b44-bdd4-d74c096bd20b",
   "metadata": {},
   "source": [
    "Likewise, we know that the row at `Index 0` has the values that is the supposed column header for the table. However, checking each of the cells in this row would make us realize that the column header for the first column should not be `Year`, but rather `Geolocation` as the values in these columns refer to the different regions. \n",
    "\n",
    "Thus, we can change the value of the first column in this row to `Geolocation`, so that we would not need to rename the column if we directly made the 0th row into the column header. Then, we can [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the row at `Index 0` as it is now unnecessary. Additionally, we can see that there is a row of **NaN**s at `Index 1`, which would become the 0th row once we drop the row that became the column headers. This should be dropped also, before the index is resetted using the [`reset_index`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c399bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0, '1.5.4 Proportion of local governments that adopt and implement local disaster risk reduction strategies in line with national disaster risk reduction strategies (Indicator can also found in SDG 13.1.3 and 11.b.2)'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63de6eb-c487-4023-ab5c-4b3551521446",
   "metadata": {},
   "source": [
    "The next step would be renaming the values under the `Geolocation`, although, as seen in the resulting table, we would notice that there is no row for **PHILIPPINES**. This is reflected in the way that we set the values of this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names [1:]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3d59e-8daf-459d-b55c-f8fecae9590b",
   "metadata": {},
   "source": [
    "As with the previous datasets, we would have to [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) the '..' and '...' values, which represents **null**, in the DataFrame with **NaN**s. This is to avoid any errors that would happen in these rows, and so that it would be represented properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3442e1-8620-4306-9c3d-5fccb0a373e4",
   "metadata": {},
   "source": [
    "After all of this, we can now transform this dataset that is in its wide represetation into its long representation using the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b770d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb0881-2638-42af-87f5-189005ec7ce4",
   "metadata": {},
   "source": [
    "Once we were able to convert it to its long representation, we would see that the column names in this new DataFrame are not descriptive with respect to the values underneath the column. Directly merging this with the combined DataFrame would make it hard for its users to distinguish what these columns are for, which is why it was [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)d to its correct column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2082e-c04a-42dd-a39d-85ad84e52db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'value':'1.5.4 Proportion of local governments that adopt and implement local disaster risk reduction strategies in line with national disaster risk reduction strategies', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f7139c-f337-445e-b8fd-27fb77486948",
   "metadata": {},
   "source": [
    "After this, we can now [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) it to the combined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc2c1d",
   "metadata": {},
   "source": [
    "#### 3.4.1. Mortality rate attributed to cardiovascular disease, cancer, diabetes or chronic respiratory disease\n",
    "To start with the fifth dataset, let us load the data from the csv file using pandas' [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1742b038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/3.4.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/3.4.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ae9cb-9d71-4002-9e76-d59718a0e57c",
   "metadata": {},
   "source": [
    "Based on the DataFrame that we got using the [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function, we can see that there are rows of **NaN**s at the lower part of the DataFrame. Upon further inspection, it started from `Index 266`, which is why the rows from this index was [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)ped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f41d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (data.index [266:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34e299-bf3a-4c7a-b1b1-ef5a53dd4ee9",
   "metadata": {},
   "source": [
    "As the column headers are all **Unnamed**, we need to set the column headers to its correct value, which is found at `Index 0`. Although, the values for the first three columns in this row are not descriptive to be column headers, which is why we are changing their values to the correct descriptive name for the rows underneath them using the [`at`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at.html) function.\n",
    "\n",
    "As we have no use for the row at `Index 0`, we can now [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) this row. With this, we would also be [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)ping the next row as it is just a row of **NaN**s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe28044",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0, '3.4.1 Mortality rate attributed to cardiovascular disease, cancer, diabetes or chronic respiratory disease'] = 'Indicator'\n",
    "data.at[0, 'Unnamed: 1'] = 'Geolocation'\n",
    "data.at[0, 'Unnamed: 2'] = 'Sex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92317517",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbc3d1-ef9e-4db6-a7fd-9ec3202680db",
   "metadata": {},
   "source": [
    "As the `Sex` column is not available for all datasets, it was decided that only the total—or those rows with **Both Sexes**—would be considered. Once we our data only includes rows with **Both Sexes** as the value of their `Sex` column, we can now [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) this column as this column would only have one unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b50ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data [data ['Sex'] == 'Both Sexes']\n",
    "data = data.drop('Sex', axis = 1)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceded53-ad50-4010-8247-5b882a6fbae7",
   "metadata": {},
   "source": [
    "Then, we need to [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) all cells that has the value of either '..' or '...' with **NaN** for better computation in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbdff86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b108e45-2b94-4be4-bc5b-b05a693b6dd8",
   "metadata": {},
   "source": [
    "Upon studying the different indicators under this specific Sustainable Development Goal (SDG), we would realize that it is comprised of different subsets: (1) cardiovascular diseases, (2) cancer, (3) diabetes, and (4) chronic respiratory disease. However, as we only aim to get the total mortality rate with respect to all of these diseases, we would only get the rows under this indicator which is from `Index 0` to `Index 16`.\n",
    "\n",
    "Then, after dividing the different subsets, we can now [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the `Indicator` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00315903-8819-4d5a-ac06-c1d1bd44a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Indicator'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254a530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = data [0:16]\n",
    "cardio_data = data [16:34]\n",
    "cancer_data = data [34:52]\n",
    "diabetes_data = data [52:70]\n",
    "respi_data = data [70:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop('Indicator', axis = 1)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1dd47-89af-4fba-b545-9c7dd95531c4",
   "metadata": {},
   "source": [
    "Upon inspection, we would realize that there are two regions that are missing from the table, which are **Region V** and **Region VI**, which is why we would only be using the region names that are included in the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no region five and six\n",
    "all_data ['Geolocation'] = region_names [0:8] + region_names [10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c8c88-559a-4805-90d4-8fede47929b9",
   "metadata": {},
   "source": [
    "After this, with the use of the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function, we can now convert our DataFrame to its long representation. Then, we must set the column headers to describe the values in this column, which is why we would need to [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72077d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.melt(all_data, id_vars='Geolocation', value_vars=all_data.columns [1:]) \n",
    "\n",
    "all_data.rename(columns = {'value':'3.4.1 Mortality rate attributed to cardiovascular disease, cancer, diabetes or chronic respiratory disease (Total data)', 0 : 'Year'}, inplace=True)\n",
    "all_data = all_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca8fe0a-89a8-444e-9a13-a17e4602d971",
   "metadata": {},
   "source": [
    "After this, we can now [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) it to the DataFrame which holds the combined datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(all_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7f027",
   "metadata": {},
   "source": [
    "#### 3.7.1. Proportion of women of reproductive age (aged 15-49 years) who have their need for family planning satisfied [provided] with modern methods\n",
    "\n",
    "Using the same [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function, we load the sixth dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c272de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/3.7.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/3.7.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79779e9",
   "metadata": {},
   "source": [
    "Irrelevant rows that are just footers for the file are also [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)ped. From the DataFrame above, we can see that these are the rows from `Index 20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d094b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (data.index [20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec07ab96",
   "metadata": {},
   "source": [
    "Additionally, we can see that the current column names are **Unnamed**. Thus, we have to set the column names to its correct values so that we can determine what the values in the columns are.\n",
    "\n",
    "Understanding the data, we can see that the row at `Index 0` holds the value for the column headers. However, there is a **NaN** value, which should be **Geolocation** based on the data underneath it. This is why the value of this cell was changed to **Geolocation** using the [`at`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at.html) function.\n",
    "\n",
    "This is done before the column names was set to the row at `Index 0`, and then [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)ping this row and the row of NaNs at the next row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a31a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0, 'Unnamed: 1'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c980c",
   "metadata": {},
   "source": [
    "Added to this, we can see that there is a column of **NaN**s, which we do not need, so we can also [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34dd347",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('Year', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd2d4c",
   "metadata": {},
   "source": [
    "Just like what we have done in the previous datasets, we would rename the **Geolocation** column based on the common names of the region for easier understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237483ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06ab1c8",
   "metadata": {},
   "source": [
    "As the missing data or null values in the dataset are represented by '..' or '...', which are strings that might affect the computations that might be done in this numerical columns, we would be using the [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) function to replace these string values to **np.nan**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5d466",
   "metadata": {},
   "source": [
    "As the dataset now looks like the wide representation that we wanted, we would be transforming it to its long representation, using the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function, so that we could merge it to the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d22f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d37f7e",
   "metadata": {},
   "source": [
    "Although, before merging it to the combined dataset, we would need to [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) the columns `0` and `value`, as they are not descriptive enough. If we directly merged it to the combined dataset, we might not be able to determine what the values in these columns mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'value':'3.7.1 Proportion of women of reproductive age (aged 15-49 years) who have their need for family planning satisfied [provided] with modern methods', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1712d",
   "metadata": {},
   "source": [
    "Once the column names have been fixed, we could use the [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) function to use outer join to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ec856",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcba981",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eff4b9",
   "metadata": {},
   "source": [
    "#### 3.7.2. Adolescent birth rate aged 15-19 years per 1,000 women in that age group\n",
    "Then, the seventh dataset could be loaded using the same [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4093d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/3.7.2.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/3.7.2.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b320b",
   "metadata": {},
   "source": [
    "As seen in the previous datasets, there are three types of columns that are processed and [`drop`]ped first: (1) the irrelevant rows that were footers in the .csv file, (2) the row that would be turned into the column headers, and (3) the row of **NaN**s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (data.index [20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d15492",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f033c",
   "metadata": {},
   "source": [
    "Although, we can see that there is a column name that does not correctly represent the data of this column: the `Year` column does not indicate years, but rather the regions. This is why it was [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)d to `Geolocation`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e03309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'Year':'Geolocation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a9fbb",
   "metadata": {},
   "source": [
    "Once we have cleaned the column headers, the values for the `Geolocation` column would be fixed to include their common names. It is important to note that it was made sure that each of the row completely match the arrangement in the `region_name` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40460ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e1a98",
   "metadata": {},
   "source": [
    "As we now have fixed the number of rows and the column names, we would now replace the string representation of null or missing vlaues. This is done with the use of [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) function, which would convert the '..' and '...' values into **np.nan**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ec64b4",
   "metadata": {},
   "source": [
    "Then, we can now convert our DataFrame into its long representation using the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function. As in the processing of the previous datasets, we would have to [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) the column names as they are not descriptive enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'3.7.2 Adolescent birth rate aged 15-19 years per 1,000 women in that age group', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638434c",
   "metadata": {},
   "source": [
    "As we are now sure that the missing or null values are correctly represented, the values of the `Geolocation` are now more easily understandable, and the column headers are descriptive enough, we can now merge this dataset into the combined datasets using the [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d1f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b834b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7175b9",
   "metadata": {},
   "source": [
    "#### 4.1.s1. Completion Rate of elementary and secondary students\n",
    "To start with the eighth dataset, let us load the data from the csv file using pandas' [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d2a3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/4.1.s1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/4.1.s1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bb5de",
   "metadata": {},
   "source": [
    "From the view of the DataFrame above, we can see that there are unnecessary rows captured by the  [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function. To be able to correctly represent the data, we would need to [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[164:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a15c4",
   "metadata": {},
   "source": [
    "Another problem that we have based on the DataFrame shown above is the lack of column names, as shown in the **Unnamed** values in the header. Studying the DataFrame, we would find the supposed column headers in the row of `Index 0`, though we face the problem of having **NaN** values at the first three columns of this row. This is why the values in these cells are changed using the [`at`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at.html) function, before converting this row to be the column header.\n",
    "\n",
    "After we have been able to turn this into the column header, we would need to drop this row and the row beneath it as they are unnecessary rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab049343",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0, '4.1.s1 Completion Rate of elementary and secondary students 1/ 2/'] = 'Geolocation'\n",
    "data.at[0, 'Unnamed: 1'] = 'Level of Education'\n",
    "data.at[0, 'Unnamed: 2'] = 'Sex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82609555-68d2-4e68-baa2-baf0ed21d707",
   "metadata": {},
   "source": [
    "Just like in datasets that has the `Sex` column, we would only be getting rows with the value for this column as **Both Sexes**. Afterwards, as we already have no need for this column anymore, we can [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data [data['Sex'] == 'Both Sexes']\n",
    "data = data.drop ('Sex', axis = 1)\n",
    "data = data.reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca917d6-45d2-4173-860e-aa719e6a7cdf",
   "metadata": {},
   "source": [
    "As we can see from the resulting dataset, there are still **NaN** values in the `Geolocation` column, which we do not want as this would be used in merging the datasets together. However, if we study it, we would realize that the reason for this is that one value for `Geolocation` actually spans to the next two rows as there are different values for the `Level of Education` column. Although, we cannot just separate the dataset per unique value of the `Level of Education` column, as the `Geolocation` would be NaN for all  **Secondary (Junior High School)** and **Secondary (Senior High School)**. \n",
    "\n",
    "Due to this, we copy the value of the `Geolocation` column of a row to the next two rows after it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the geolocation value to the next two rows\n",
    "i = 0\n",
    "while i < len (data):\n",
    "    if i % 3 == 0:\n",
    "        data.at[i + 1, 'Geolocation'] = data['Geolocation'][i]\n",
    "        data.at[i + 2, 'Geolocation'] = data['Geolocation'][i]\n",
    "        i = i + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416b09e-0657-4c54-8eab-dc7c4717fbd2",
   "metadata": {},
   "source": [
    "Before we divide the dataset based on the value of `Level of Education`, we must first replace cells with the strings '..' or '...' with **np.nan**. This is so that we would not need to process this representation of missing or null values separately (i.e., per division). Then, we can now separate them so that we can properly label it before merging it to the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd134861",
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_data = data [data['Level of Education'] == 'Elementary']\n",
    "elem_data = elem_data.reset_index (drop=True)\n",
    "\n",
    "junior_data = data [data['Level of Education'] == 'Secondary (Junior High School)']\n",
    "junior_data = junior_data.reset_index (drop=True)\n",
    "\n",
    "senior_data = data [data['Level of Education'] == 'Secondary (Senior High School)']\n",
    "senior_data = senior_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90255f5-add8-4dc4-9579-4b734b55fd82",
   "metadata": {},
   "source": [
    "Once we have successfully divided the dataset based on the value of the `Level of Education` column, we can now [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) this column as each of the division would technically only have one value for this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_data = elem_data.drop ('Level of Education', axis = 1)\n",
    "elem_data = elem_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_data = junior_data.drop ('Level of Education', axis = 1)\n",
    "junior_data = junior_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_data = senior_data.drop ('Level of Education', axis = 1)\n",
    "senior_data = senior_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f064c7-ce57-4dd7-9e1d-7f34eeff831e",
   "metadata": {},
   "source": [
    "After making sure that the arrangement of the region matches the arrangement of the values of the `region_names` variable, we can change the values of the `Geolocation` column for each of the division. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca0b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c33ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673be70d-9e0d-4288-b018-7c83961b5d0a",
   "metadata": {},
   "source": [
    "Then, we can now convert the DataFrames into their long representation, before using the [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) function to make the column names more descriptive of the data in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5de434",
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_data = pd.melt(elem_data, id_vars='Geolocation', value_vars=elem_data.columns [1:]) \n",
    "\n",
    "elem_data.rename(columns = {'value':'4.1.s1 Completion Rate of elementary and secondary students 1/ 2/ (Elementary)', 0 : 'Year'}, inplace=True)\n",
    "elem_data = elem_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_data = pd.melt(junior_data, id_vars='Geolocation', value_vars=junior_data.columns [1:]) \n",
    "\n",
    "junior_data.rename(columns = {'value':'4.1.s1 Completion Rate of elementary and secondary students 1/ 2/ (Junior High School)', 0 : 'Year'}, inplace=True)\n",
    "junior_data = junior_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67c323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "senior_data = pd.melt(senior_data, id_vars='Geolocation', value_vars=senior_data.columns [1:]) \n",
    "\n",
    "senior_data.rename(columns = {'value':'4.1.s1 Completion Rate of elementary and secondary students 1/ 2/ (Senior High School)', 0 : 'Year'}, inplace=True)\n",
    "senior_data = senior_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1dce58-3100-4866-abda-b4f3dd58c559",
   "metadata": {},
   "source": [
    "As we have now made sure that each of division would be understandable even if combined with the combined dataset, we can now [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) each of them into the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(elem_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(junior_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c673f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(senior_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4a9b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dbdc54",
   "metadata": {},
   "source": [
    "#### 4.c.s2. Number of Technical-Vocational Education and Training (TVET) trainers trained\n",
    "Next, we can load the ninth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa557a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/4.c.s2.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/4.c.s2.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c76129-8a09-4c42-bc88-1e6b740cded3",
   "metadata": {},
   "source": [
    "As usual, we would first be [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)ping the irrelevant rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf021a0-4bf3-430e-9a69-2718b508c3dc",
   "metadata": {},
   "source": [
    "Then, as we know that the correct column headers are found at `Index 0`, we have to fix the values of this row to fully represent the data in the columns. This is why the **Year** value was changed into **Geolocation** because the values in this column are the rows of the country.\n",
    "\n",
    "After this, we can now make the value of this row as the value of the column headers, before [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)ping this row as it would not be used anymore. In line with this, we can also [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the row of **NaN**s underneath this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0, '4.c.s2 Number of Technical-Vocational Education and Training (TVET) trainers trained'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77dfadf-283a-4564-8d32-e918493e86e8",
   "metadata": {},
   "source": [
    "Then, we need to change the values of the `Geolocation` column to match the prescribed format for the region names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04dce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f7e2f1-19e8-4c4c-a5fe-d86b6683ca5c",
   "metadata": {},
   "source": [
    "After this, we need to clean the dataset by turning the string representation of missing or null values, which are '..' and '...', into **np.nan**. This would allow us to correctly use mathematical functions into these columns without errors arising due to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f000334",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458f1b9-b0ad-4f8f-ab48-f01a13c3b89a",
   "metadata": {},
   "source": [
    "Once we have done this, we can convert the DataFrame into its long representation, which would allow us to merge it with the combined dataset. Converting a DataFrame that is in its wide representation into its long representation is made possible by the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function.\n",
    "\n",
    "However, using the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function would result into a three-column DataFrame which has the following column names: (1) `Geolocation`, (2) `0`, and (3) `value`. The last two columns are not properly descriptive of the values of the column, which is why these two columns are [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)d. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'4.c.s2 Number of Technical-Vocational Education and Training (TVET) trainers trained', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08405c65-013f-4e05-a9c1-c5a0650319f9",
   "metadata": {},
   "source": [
    "As we now have a DataFrame that is in its long representation, we can now [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) it to the combined DataFrame, with respect to the values of the `Geolocation` and `Year` columns. This means that a row from this DataFrame would be [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html)d into the combined dataset on the row that has the same `Geolocation` and `Year`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49135c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f509cc-0456-44aa-a64d-965cc909cd4d",
   "metadata": {},
   "source": [
    "#### 7.1.1. Proportion of population with access to electricity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e5d01-9f61-4dfd-af21-600f7c2f8700",
   "metadata": {},
   "source": [
    "Now, we will proceed to loading the tenth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac594cf-64f2-48bc-a6af-ad8ae14619b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/7.1.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/7.1.1.csv') // AJ TO DO\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776c44f-e095-4d65-b9d5-1d408318e7c2",
   "metadata": {},
   "source": [
    "Before anything else, we drop the irrelevant rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518a796-142c-4d45-ae75-d3538d27525e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop(data.index[20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad23de-e28d-4daf-be1a-19572513b55e",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we will change the data in Index 0 at column '7.1.1 Proportion of population with access to electricity 1/' into 'Geolocation' since our goal is to make the geolocation the first column of the dataframe. By doing this, Index 0 has now the correct column headers. \n",
    "\n",
    "With this, we have to arrange the values of this row to fully represent the data in the columns. Therefeore, we will now make the value of this row as the value of the column headers. \n",
    "\n",
    "After this, we drop this row (Index 0) as it would not be used anymore as well as the row of NaNs underneath this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120e492-b2f2-473a-b146-ed735e1a0314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.at[0,'7.1.1 Proportion of population with access to electricity 1/'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74492c5f-0979-4394-9ba9-aef718f97d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b247c55-d8b1-47f7-a3c9-28e653b6ddb9",
   "metadata": {},
   "source": [
    "After checking if the order of the Geolocation is the same as what we intended, we will initialize the Geolocation column of the region names to make sure that the format of the region names in this dataset is the same as the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34f233-20bb-42f4-b634-eb6595a227a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2475af-6a5f-4eca-9629-03f000144826",
   "metadata": {},
   "source": [
    "We will then change the the '..' or '...' strings to NaN using the np.nan. Again, these NaN values were not dropped because all years from 2001-2022 will be in the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025dea61-9107-4683-b2bd-12d016ef480f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b1cdca-8f36-4d2d-afb4-7abb0fa64fa7",
   "metadata": {},
   "source": [
    "We can now convert the DataFrame into its long representation using the `melt` function. This would allow us to merge it with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6bee18-27b1-4d60-a5af-4e3c90ab024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'7.1.1 Proportion of population with access to electricity 1/', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8b9ad-444e-4fc3-bdaa-bbfc8c3be189",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a99ba1-4579-4fa8-9855-8facc65293c8",
   "metadata": {},
   "source": [
    "We will now combine this dataset to the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b73f9-ce13-4da4-bd99-1550a052db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeed33a-43fc-46f4-b325-030535f6e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc1cc6-45e6-4a8f-8187-9b033e95b49e",
   "metadata": {},
   "source": [
    "#### 8.1.1. Annual growth rate of real GDP per capita"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983eba7-a81a-40d2-8691-fe5567cab1aa",
   "metadata": {},
   "source": [
    "Loading the eleventh dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dde51a-d476-4fa9-bc07-8433f48227b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/8.1.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/8.1.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304fa29-87d1-40ae-89dd-98930bbc54f4",
   "metadata": {},
   "source": [
    "We will now drop the irrelevant rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ee564-13e6-4787-952b-d47fea3320b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cdb8c-7c92-49ac-a36b-32cd1ae6052e",
   "metadata": {},
   "source": [
    "Observing the header column and the Index 0, the data in the Index 0 is much more similar to the column names we want for the dataset. With this, it would be more hassle to change all columns names in the header column than changing the data in Index 0 and setting it to be the header column.\n",
    "\n",
    "With this, we will change the data in at Index 0 Column 0 into 'Geolocation' since our goal is to make the geolocation the first column of the dataframe. By doing this, Index 0 has now the correct column headers. Then, will now make the value of this row as the value of the column headers.\n",
    "\n",
    "After this, we drop this row (Index 0) as it would not be used anymore as well as the row of NaNs underneath this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa413ad-5af9-4cc1-80ab-7ec19f626c19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.at[0,'8.1.1 Annual growth rate of real GDP per capita'] = 'Geolocation'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597ac77-0cb8-4d3a-8167-b914275c971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f928e9-962c-476b-9397-4c2381400a93",
   "metadata": {},
   "source": [
    "After checking if the order of the Geolocation is the same as what we intended, we will initialize the Geolocation column of the region names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d656e1d-e29a-4d5d-aa6e-154ba3cae441",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05eda14-c8ba-4ab7-8406-0a1e65fa6e40",
   "metadata": {},
   "source": [
    "To represent the missing values clearly, we change the the '..' or '...' strings to NaN using the np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82542b29-d9eb-4c9e-9b9c-1e3f431126ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35a0c5-c768-49eb-a45f-fd677db1fc5f",
   "metadata": {},
   "source": [
    "We can now convert the DataFrame into its long representation to allow us to merge it with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe63cf-da54-446a-a186-896e6eb300e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'8.1.1 Annual growth rate of real GDP per capita', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaad9fc-8f5b-46c1-a331-bb4711e16f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85581d2-5d44-4eff-81dc-2462969c695c",
   "metadata": {},
   "source": [
    "After this, we combine this dataset with the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae274e88-d544-436f-a8fa-433eb3c4ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c93d29-937a-42ef-89df-fc47c857ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975d192-f4fb-4af6-a9dc-506543515668",
   "metadata": {},
   "source": [
    "#### 10.1.1. Growth rates of household expenditure or income per capita among the bottom 40 per cent of the population and the total population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d487c550-6aa4-4971-915c-dcc2653f87dc",
   "metadata": {},
   "source": [
    "Loading the twelfth dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26902028-d532-4665-b9dc-0f2defadfa08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/10.1.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/10.1.1.csv') \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12158073-ea7a-4ec3-87e5-3410c1dd8491",
   "metadata": {},
   "source": [
    "Dropping the irrelevant rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883527c-4d33-4d2f-a864-bcb14412a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[38:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd7767-fa5b-4f10-a7ef-fd827a7d7ea6",
   "metadata": {},
   "source": [
    "We will change the data in at Index 0 Column 0 into 'Geolocation' since our goal is to make the geolocation the first column of the dataframe. By doing this, Index 0 has now the correct column headers. Then, will now make the value of this row as the value of the column headers.\n",
    "\n",
    "After this, we drop this row (Index 0) as it would not be used anymore as well as the row of NaNs underneath this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e8e53b-c32a-4e25-9907-6af9a1a8b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0,'10.1.1 Growth rates of household expenditure or income per capita among the bottom 40 per cent of the population and the total population'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a581be-c6fd-49ff-bab4-92abb818c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ee2a7-6792-413d-83eb-923b4e3e4500",
   "metadata": {},
   "source": [
    "To represent the missing values clearly, we change the the '..' or '...' strings to NaN using the np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b103940-b966-401a-b64f-4ff55b04c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db70c3-e15a-4143-9bf9-331048a01822",
   "metadata": {},
   "source": [
    "As observed in this dataset, we have two parts which are **10.1.1.1 Bottom 40 percent of the population** and **10.1.1.2 Total Population**. Since we will both need these parts, we will still get both parts to combine with other datasets. However, we will divide them into two different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9ca5d-0bc4-40ff-9a48-a4bde69eeb45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['Geolocation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759f09c-470b-4dac-a72a-9c3a1c54e338",
   "metadata": {},
   "source": [
    "**10.1.1.1 Bottom 40 percent of the population** goes to `bottom_popu_data` while **10.1.1.2 Total Population** goes to `total_popu_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1a8d9-6807-4e99-9844-2287e9c7a7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bottom_popu_data = data [0:18]\n",
    "total_popu_data = data [18:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375860bd-46fb-4b21-aa2c-57ad57aac83d",
   "metadata": {},
   "source": [
    "Since `total_popu_data` started with index 18, we will set its starting index to 0. \n",
    "\n",
    "Also, since the first row of each of the parts is a record for the Philippines and the order of the geolocation of each DataFrame is correct, we will initialize it with the region_names for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ace7bc-5c95-419a-adc7-1e8902bda937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_popu_data = total_popu_data.reset_index (drop=True)\n",
    "\n",
    "bottom_popu_data ['Geolocation'] = region_names\n",
    "total_popu_data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a79861-9fc0-4a7c-99d4-2d3d7742ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_popu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1d929-f5f9-4f30-b00f-cc6e72ef4158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_popu_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86675795-19df-4b59-bfe6-cff9301e01a4",
   "metadata": {},
   "source": [
    "We can now convert both DataFrames into their long representation to allow us to merge both of them with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946d5f2-b838-4c4b-b0fe-d3bf8be0e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_popu_data = pd.melt(bottom_popu_data, id_vars='Geolocation', value_vars=bottom_popu_data.columns [1:]) \n",
    "\n",
    "bottom_popu_data.rename(columns = {'value':'10.1.1.1 Bottom 40 percent of the population', 0 : 'Year'}, inplace=True)\n",
    "bottom_popu_data = bottom_popu_data.astype({'Year':'int'})\n",
    "\n",
    "total_popu_data = pd.melt(total_popu_data, id_vars='Geolocation', value_vars=total_popu_data.columns [1:]) \n",
    "\n",
    "total_popu_data.rename(columns = {'value':'10.1.1.2 Total Population', 0 : 'Year'}, inplace=True)\n",
    "total_popu_data = total_popu_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5a1e8-1be6-4cbc-9b34-1869a5b66be0",
   "metadata": {},
   "source": [
    "Combining the two different datasets with the currently combined data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c34890-dde5-4f91-892e-2c5b76c836ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the 10.1.1.1 dataset with the current combined dataset\n",
    "combined_data = combined_data.merge(bottom_popu_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)\n",
    "# Adding the 10.1.1.2 dataset with the current combined dataset\n",
    "combined_data = combined_data.merge(total_popu_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4663fb12-d5ec-4d2d-85cc-c9601cc120b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f512849-a954-4168-90e7-1286086a1ed4",
   "metadata": {},
   "source": [
    "#### 14.5.1. Coverage of protected areas in relation to marine areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603e52b-0774-49a2-b504-4626e5db4834",
   "metadata": {},
   "source": [
    "We will now read the thirteenth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426094d2-f3bb-425c-ac17-0fecb4b318b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/14.5.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/14.5.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19dd3b5-894f-4c18-8cab-3cd81e81332d",
   "metadata": {
    "tags": []
   },
   "source": [
    "As usual, we drop the irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0718d0-5866-452e-a12c-9ec881a87959",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (data.index [38:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bc2d75-6f57-44c4-bfdc-03f263717c03",
   "metadata": {},
   "source": [
    "We will now edit the data in column 0 & 1 at Index 0 to make the whole Index 0 look like the column headers we want.Then, we set the Index 0 to become the header columns. After this, we drop the Index 0 and the row of NaNs underneath it.\n",
    "\n",
    "The first column was renamed to `Indicator` as it contains the names of the parts in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11feeaf-d55a-4ddf-8448-5ce99e6b6957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.at[0, '14.5.1 Coverage of protected areas in relation to marine areas'] = 'Indicator'\n",
    "data.at[0, 'Unnamed: 1'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba711b8-684a-4a21-b299-62bc0e9fa2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3493583d-2836-4826-9f62-2c8b56e20236",
   "metadata": {},
   "source": [
    "To represent the missing values clearly, we change the the '..' or '...' strings to NaN using the np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ea91f-6dbd-46c6-9e23-7dd99dcbf73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2c553-81b9-4a60-b6c8-93d2be67e3e4",
   "metadata": {},
   "source": [
    "As observed in this dataset, we have two parts which are **14.5.1.1 Coverage of protected areas in relation to marine areas, Universe (in million hectares)** and **14.5.1.2 Coverage of protected areas in relation to marine areas, NIPAS ans Locally managed MPAs 1/**. Since we will both need these parts, we will still get both parts to combine with other datasets. However, we will divide them into two different datasets.\n",
    "\n",
    "For this, we will retain the `Indicator` column first, which contains the name of the parts, for identifying how this dataset will be divided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dad953-4e3f-4533-9c9b-2b23be581288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['Indicator'].unique()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad04ca-c19a-4deb-b227-0dfa8fcbcf5f",
   "metadata": {},
   "source": [
    "**14.5.1.1 Coverage of protected areas in relation to marine areas, Universe (in million hectares)** goes to `universe_data` while **14.5.1.2 Coverage of protected areas in relation to marine areas, NIPAS ans Locally managed MPAs 1/** goes to `nipas_data`. \n",
    "\n",
    "Since the `nipas_data` will start at Index 18, we will reset it to Index 0 after the division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34971e-5d5b-4450-bf0f-67d1c3ab227d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "universe_data = data [0:18]\n",
    "nipas_data = data [18:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d43d91-c8a0-4d15-b345-94d4609b833f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nipas_data = nipas_data.reset_index (drop=True)\n",
    "nipas_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d7933-8e69-4052-8306-868165c1a434",
   "metadata": {},
   "source": [
    "Since the dividing of the dataset is done, we won't be needing the `Indicator` column anymore. Therefore, we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d8dc9-4cc3-4a55-a3f9-0f8784400cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "universe_data = universe_data.drop('Indicator', axis = 1)\n",
    "nipas_data = nipas_data.drop('Indicator', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f5fbad-3be1-4aa9-90ef-f6e9b9348f99",
   "metadata": {
    "tags": []
   },
   "source": [
    "Since the order of the geolocation of each DataFrame is correct, we will initialize it with the region_names for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e3577-b5d4-49f5-82d6-518eea3f1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_data ['Geolocation'] = region_names\n",
    "nipas_data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0c8095-939a-44a0-9707-d80a22863d1a",
   "metadata": {},
   "source": [
    "We can now convert both DataFrames into their long representation to allow us to merge both of them with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c34de-32cd-4a4b-ab02-71f1e2147914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 14.5.1.1\n",
    "universe_data = pd.melt(universe_data, id_vars='Geolocation', value_vars=universe_data.columns [1:]) \n",
    "universe_data.rename(columns = {'value':'14.5.1.1 Coverage of protected areas in relation to marine areas, Universe (in million hectares)', 0 : 'Year'}, inplace=True)\n",
    "universe_data = universe_data.astype({'Year':'int'})\n",
    "# 14.5.1.2\n",
    "nipas_data = pd.melt(nipas_data, id_vars='Geolocation', value_vars=nipas_data.columns [1:]) \n",
    "nipas_data.rename(columns = {'value':'14.5.1.2 Coverage of protected areas in relation to marine areas, NIPAS ans Locally managed MPAs 1/', 0 : 'Year'}, inplace=True)\n",
    "nipas_data = nipas_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82564de9-cd17-47ca-a37c-960b1dff7ef2",
   "metadata": {},
   "source": [
    "Combining the two different datasets with the currently combined data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80436a8b-6f4e-4400-9f83-29dd57810434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding the 14.5.1.1 dataset with the current combined dataset\n",
    "combined_data = combined_data.merge(universe_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)\n",
    "# Adding the 14.5.1.2 dataset with the current combined dataset\n",
    "combined_data = combined_data.merge(nipas_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418dec9b-2732-4eb4-8afd-716405423184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77579d29-6c7f-4573-8898-45bb1bebce13",
   "metadata": {},
   "source": [
    "#### 16.1.1 Number of victims of intentional homicide (per 100,000 population)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4987f6f-acbf-44eb-b20d-555b7eae1311",
   "metadata": {},
   "source": [
    "We will now load the fourteenth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9901d8-d514-47b2-b0b1-8075b5083806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/16.1.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/16.1.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d69677-496b-4ef3-985f-08e49413c2a4",
   "metadata": {},
   "source": [
    "Now, we will drop the irrelevant rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb31169-67ec-4a42-b55c-74fae3453f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ea885-94d6-4015-b88a-b90e41fa3b5b",
   "metadata": {},
   "source": [
    "Since Index 0 is almost the same as the column header we want, we will just change the content in the first column to `Geolocation`. This also because the column already containes the regions of the Philippines.\n",
    "\n",
    "Then, we set the Index 0 to become the header column. After this, we will drop Index 0 and the rows of NANs underneath it since we will not be needing this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac413ccf-99b3-4e74-a839-80f4e0c153a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0,'16.1.1 Number of victims of intentional homicide (per 100,000 population) 1/'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420d36b-a678-428b-ae2a-dc80a16ab4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1459fdd-ac78-4c70-83bc-c7604a8ea1f6",
   "metadata": {},
   "source": [
    "We will now check the order of the Geolocation if it is the same as the combined dataset. Then, to make the naming of Geolocation uniformed, we will initialized the Geolocation with region_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41edc841-eb4b-443b-8b7e-a58f935cd3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3a4e3-ce4f-4e34-b1f4-be8049693f54",
   "metadata": {},
   "source": [
    "We will then change the the '..' or '...' strings to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe43386-eec0-4102-b921-6a3593fa7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3690d43-65d2-4886-be8c-2250d2d0fb81",
   "metadata": {},
   "source": [
    "We can now convert the DataFrame into its long representation using the melt function. This would allow us to merge it with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8a630-0148-45ce-becf-351cf55cc557",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'16.1.1 Number of victims of intentional homicide (per 100,000 population) 1/', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac79f82-6dca-4263-a702-04a8a1ca1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52428452-d34c-4433-a171-651a1f3bc293",
   "metadata": {},
   "source": [
    "We will now combine this dataset to the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2f358-24f1-41b5-a710-118f2ee019da",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe202a-11d3-4c3a-b885-d75ba1b24b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f65a94-cafe-4eee-8529-8cbc684a9632",
   "metadata": {},
   "source": [
    "#### 16.1.s1 Number of murder cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec4dbf-3dc2-4966-baa6-c37857f85f5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We are now loading our fifteenth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454e609-1640-4ba5-b679-97bd2245f2d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/16.1.s1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/16.1.s1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016856f1-52c4-4f29-91ce-1fdba303f576",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "Dropping the irrelevant rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc8faf9-b0e0-4d4e-a7ca-3f198e317402",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccd718-7ba4-4400-bdd9-283901ee3d98",
   "metadata": {},
   "source": [
    "Since Index 0 is almost the same as the column header we want, we will just change the content in the first column to \"Geolocation\". Then, we set the Index 0 to become the header column. After this, we will drop Index 0 and the rows of NANs underneath it since we will not be needing this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ef585-e5b6-4698-b8bd-15f6c8e64d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0,'16.1.s1 Number of murder cases'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b37aab-f004-417a-8af2-77e49bfd77ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93eec99-02a3-4a9d-a552-51d5c08bc52f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We will now check the order of the Geolocation if it is the same as the combined dataset. Then, to make the naming of Geolocation uniformed, we will initialized the Geolocation with region_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc8a75-03a5-4706-b16d-faa82ee18528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ad2a0-3246-48f0-8c8c-6e779c207319",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will then change the the '..' or '...' strings to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f42646-86ca-4fce-a42a-d1813be5b61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793827cf-f942-4d72-86c8-baeb451bb81d",
   "metadata": {},
   "source": [
    "We can now convert the DataFrame into its long representation using the melt function. This would allow us to merge it with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b195c05-23e7-499a-b8cc-d59b1100bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'16.1.s1 Number of murder cases', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b7890-3618-4dec-b9f5-90598f2402b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac97fc49-acb0-4569-baf7-bd7a4dfa9a09",
   "metadata": {},
   "source": [
    "We will now combine this dataset to the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46166faa-1900-4cee-b13d-3e38ee490b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7a52c-79b3-46b8-93a1-331408ffa0c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732fc967-f688-4a87-a209-a8229eb7413c",
   "metadata": {},
   "source": [
    "#### Other Non-SDG datasets that can help us in exploring the former datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1a934-e3a8-458a-92bb-3519160ca439",
   "metadata": {},
   "source": [
    "##### Changes in Inventories, by Region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80f6924-8201-414c-ad25-0c03722726dd",
   "metadata": {},
   "source": [
    "Loading the sixteenth dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90053c8b-4c3f-466f-9f11-316ea402784b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Changes in Inventories, by Region.csv', header=1, delimiter=\";\")\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/Changes in Inventories, by Region.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef9662-1ded-4d14-90ee-507aec90f359",
   "metadata": {},
   "source": [
    "Since we will be only needng the current prices and we will not be focusing on comparing each record to 2018, we will be dropping the columns with `At Constant 2018 Prices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff48bc2-a571-40c3-987b-274d548ee598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.drop(data.iloc[:, 22:], inplace=True, axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed8feb-b9dc-4f37-be62-328e7d4cbf5d",
   "metadata": {},
   "source": [
    "Also, since the ordering of the Geolocation is different in this dataset, we will be rearranging the rows based on the order of the Geolocation in `region_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1669da-fb7d-4f6c-a273-3bb76f48d387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.reindex(index=[17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6784ecd-5c56-4f4b-b63b-ec773985137b",
   "metadata": {},
   "source": [
    "Then, we will proceed to reindexing the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb721bd2-759e-465c-8fa7-d0ba0b99421f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286dae38-1769-45e0-9b21-f761f80c9c71",
   "metadata": {},
   "source": [
    "After this, we will now change the columns names: (1) `Region` to `Geolocation`, (2) `At Current Prices <Year>` to `<Year>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4cf13-ce10-42b1-ac32-8bc5be6d801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['Geolocation', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008',\n",
    "               '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017','2018', '2019', '2020']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12902b83-578f-4956-bf37-e686752e7bbb",
   "metadata": {},
   "source": [
    "After this, we will insert the region_names in the Geolocation column so that the format of the region_names will fit the ones in the combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37411f8-997e-40cf-97ac-5c20a846d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc6a429-2b2e-4ead-9640-6df40931b76e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "We will then change the the '..' or '...' strings to NaN. This is to represent the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636783f9-1af4-4adf-8699-cd0264a4b823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac773e5a-d917-4e62-8ab6-ec2140660d21",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We can now convert the DataFrame into its long representation using the melt function. This would allow us to merge it with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695827e1-ce5a-462b-9708-539afad1ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "data.rename(columns = {'value':'Changes in Inventories, by Region', 'variable' : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af16dd3-e89c-4e7f-b6ef-d1929274d316",
   "metadata": {},
   "source": [
    "Finally, we will now combine this dataset to the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c88b0-1cd9-427f-913f-2763d7663760",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c0452-9a4e-4d63-bb90-0a978531bcc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c3fc1-2059-4ec9-a9b1-df3b1f2ad860",
   "metadata": {},
   "source": [
    "##### Current Health Expenditure by Region, Growth Rates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db8f0",
   "metadata": {},
   "source": [
    "Loading the seventeenth dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d8048-ca97-4fa6-ab15-caefd531ba28",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Current Health Expenditure by Region, Growth Rates.csv', header=1, delimiter=\";\")\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/Current Health Expenditure by Region, Growth Rates.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f41b7-98fe-4a07-95d9-e798111c718b",
   "metadata": {},
   "source": [
    "Since we will only need the data nationwide and per region, we will drop the `Index 0` which contains the Total Current Health Expenditure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4475650-d691-4080-8c60-aff9302a7647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d722f5-9f2c-45c5-bec2-f1ab692c5f78",
   "metadata": {},
   "source": [
    "Also, since the ordering of the Geolocation is different in this dataset, we will be rearranging the rows based on the order of the Geolocation in region_names. After this, we will reset the index again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47acbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.reindex(index=[17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af9c27-732d-4868-b220-53ff78b6bd32",
   "metadata": {},
   "source": [
    "After this, we will now change the columns names: (1) `Region` to `Geolocation`, (2) `At Current Prices <Year>` to `<Year>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a0522-15e7-439e-a39c-63e84ee298cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.columns = ['Geolocation', '2014', '2015', '2016', '2017','2018', '2019']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac5748-1f40-491b-9b64-890068a24293",
   "metadata": {},
   "source": [
    "As observed, this dataset does not have the records for the years: 2000-2013 and 2020-2022. To allow this dataset to merge with the currently combined dataset easily, we add additional columns for representing the missing years in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab1c31-d28a-45b0-b651-183bd486fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding columns 2000-2013\n",
    "col = 1\n",
    "for i in range(2000,2014):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1\n",
    "# For adding columns 2020-2022\n",
    "col = 21\n",
    "for i in range(2020,2023):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88579f-20ea-4bd3-8516-c92f975af7dd",
   "metadata": {},
   "source": [
    "After this, we will insert the region_names in the Geolocation column so that the format of the region_names will fit the ones in the combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54f802-2186-4fd1-8506-f7cb81d93887",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b993a-1094-4118-9379-32656a4635d6",
   "metadata": {},
   "source": [
    "We will then change the the '..' or '...' strings to NaN. This is to represent the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2b84b-c1f8-45a9-a560-18d51e64368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337fc47-f540-46da-baef-2d5b82732db8",
   "metadata": {},
   "source": [
    "We can now convert the DataFrame into its long representation using the melt function. This would allow us to merge it with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870d20d-881e-43b0-b3c9-36a8e8f2b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "data.rename(columns = {'value':'Current Health Expenditure by Region, Growth Rates', 'variable' : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5320f21b-ead0-4334-aae4-34c8d920c421",
   "metadata": {},
   "source": [
    "Finally, we will now combine this dataset to the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c334912-6276-44ca-9e76-d4292838e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a0a0e-5a5d-472b-85ad-282f92321931",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2731e92-dcb5-40bc-98ad-991054dd8da4",
   "metadata": {},
   "source": [
    "##### Current Health Expenditure by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a44fe-c168-42ec-bfc2-3bc6a71428db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Current Health Expenditure by Region.csv',header = 1,sep = ';')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/Current Health Expenditure by Region.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop total current health expenditure\n",
    "data = data.drop (data.index[0])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b25ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#remove '..' and 'r'\n",
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data.columns = data.columns.str.replace('[r]', '',regex = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4baf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make nationwide index 0\n",
    "data = data.iloc[np.arange(-1, len(data)-1)]\n",
    "data = data.reset_index()\n",
    "data.drop('index', axis = 1,inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afdfae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9bf88-f81d-4a76-b9bb-0cacbc32ed06",
   "metadata": {},
   "source": [
    "To follow the format of the combined dataset and to make combining dataset easier, we add columns for years: `2000-2013` and `2020-2022`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf130287-4ac0-4162-929d-46d30cce9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding columns 2000-2013\n",
    "col = 1\n",
    "for i in range(2000,2014):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1\n",
    "# For adding columns 2021 and 2022\n",
    "data.insert(22, 2021, np.nan, True)\n",
    "data.insert(23, 2022, np.nan, True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb90a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'Current Health Expenditure by Region', 'variable' : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9173cc-5b50-4903-bfe6-1e38da48a362",
   "metadata": {},
   "source": [
    "Finally, we will now add this dataset with the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136cca4-5e61-4497-b5f2-077dfb33290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff71659-5570-41c7-bfc6-55297c52375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d6a4d-d8d0-4198-ba6f-1eb208f19326",
   "metadata": {},
   "source": [
    "##### Government Final Consumption Expenditure, by Region, Growth Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec5697-b9b1-4a3d-805d-c9104f5e312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Government Final Consumption Expenditure, by Region, Growth Rates.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove '..' and arrange row\n",
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data = data.iloc[np.arange(-1, len(data)-1)]\n",
    "data = data.reset_index()\n",
    "data.drop('index', axis = 1,inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32be0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.iloc[:, 21:41], inplace = True, axis = 1)\n",
    "  \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1d698-f683-4d98-9bb7-835dcfec9f52",
   "metadata": {},
   "source": [
    "##### Government Final Consumption Expenditure, by Region, Percent Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaae661-4fa9-4c34-af40-ce4aa383700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Government Final Consumption Expenditure, by Region, Percent Share.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813dec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove '..' and arrange row\n",
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data = data.iloc[np.arange(-1, len(data)-1)]\n",
    "data = data.reset_index()\n",
    "data.drop('index', axis = 1,inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d59821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0985036",
   "metadata": {},
   "source": [
    "##### Gross Capital Formation, by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b97f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Gross Capital Formation, by Region.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data = data.iloc[np.arange(-1, len(data)-1)]\n",
    "data = data.reset_index()\n",
    "data.drop('index', axis = 1,inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7c798",
   "metadata": {},
   "source": [
    "##### Gross Regional Domestic Product, by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ba214",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Gross Regional Domestic Product, by Region.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d462dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data = data.iloc[np.arange(-1, len(data)-1)]\n",
    "data = data.reset_index()\n",
    "data.drop('index', axis = 1,inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb38537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96552b55-efd9-441e-8200-ebb9928dd645",
   "metadata": {},
   "source": [
    "##### Population, by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68896e-e099-4579-b990-5dadf0dc0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Population, by Region.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0443f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data = data.iloc[np.arange(-1, len(data)-1)]\n",
    "data = data.reset_index()\n",
    "data.drop('index', axis = 1,inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'Population', 'variable' : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc4fab-a350-40ac-8c9b-8e969d201340",
   "metadata": {},
   "source": [
    "##### Primary Drop-out rates by Region, Sex and Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e6627-1069-4329-982e-6dfff30e4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Primary Drop-out rates by Region, Sex and Year.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1441d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.iloc[:, 11:31], inplace = True, axis = 1)\n",
    "  \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f000325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.map(lambda x: x.lstrip('Both Sexes '))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'Drop-out rate', 'variable' : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6addc98-c67d-4dc8-917c-16bf20d9ac60",
   "metadata": {},
   "source": [
    "##### Quarterly Producer Price Index for Agriculture (First Quarter 2018 to Third Quarter 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b8a54-63cc-497e-8f27-b79dd95555dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Quarterly Producer Price Index for Agriculture (2018=100) _ First Quarter 2018 to Third Quarter 2021.csv',header = 1,sep = ';')\n",
    "data[data['Commodity'] == 'AGRICULTURE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff80306",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data['Commodity'] = data['Commodity'].map(lambda x: x.lstrip('..'))\n",
    "data['Commodity'] = data['Commodity'].map(lambda x: x.lstrip('….'))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns =['2021 Fourth Quarter (Oct-Dec)', '2021 Average (Jan-Dec)'],inplace = True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598039d-dd32-4c86-a557-f9154f9170f2",
   "metadata": {},
   "source": [
    "## Data Cleaning and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2297e4c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Dropping of rows that has all `NaN` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.dropna(axis = 0, thresh = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee4d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37508d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2f0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in combined_data.columns.difference(['Geolocation', 'Year']):\n",
    "    if combined_data[x].dtypes != 'float64':\n",
    "        combined_data.loc[:, x] = combined_data[x].astype(float, errors = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b4244",
   "metadata": {},
   "source": [
    "### Cleaning of Each Columns\n",
    "#### 1.2.1. Proportion of population living below the national poverty line\n",
    "For this column, we would be using the [`describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) function in order to check if we have an outliner. This is due to the fact that we are expecting a value of 0 to 10, as we are talking about proportion or percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4802b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['1.2.1'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a85c0-6464-4201-8f7b-2fd795a0a756",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbfcc06-2999-49e2-a540-0a3418816aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7ad53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
