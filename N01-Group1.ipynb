{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1efe2220",
   "metadata": {},
   "source": [
    "# Progress of the Philippines' Sustainable Development Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5cba3",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ef5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db140c",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "The following **csv** files used in this project are acquired through a request sent to the Knowledge Management and Communications Division of the Philippine Statistics Authority.\n",
    "\n",
    "### Combining the Datasets \n",
    "In this stage, the separate datasets underwent pre-processing and cleaning before they are combined together. \n",
    "\n",
    "First, the irrelevant rows were dropped first. These were the rows that have all NaN values and the additional rows (i.e., note rows, “Data available” rows) found in the CSV files. \n",
    "\n",
    "Second, since the first row of the CSV files was the name of the indicator and unnamed rows, the resulting DataFrame had “Unnamed” as its column header. Due to this, we had to set the column headers to the second row of the DataFrame, and then drop this afterward.\n",
    "\n",
    "Third, since the `Geolocation` column would be used later to merge the datasets, the values in this column were standardized into the format `Region n: region_name`, where *n* is the corresponding region number and *region_name* is the name of the region. If it does not have a region number, then it was formatted as `region_abbreviation: region_name`, where *region_abbreviation* is its official abbreviation. \n",
    "\n",
    "Fourth, there are datasets that had divisions for a region and year, but still include a cumulative value for that division (e.g., datasets that are also divided per `Sex`, while having a value of “Both Sexes”. For this situation, we have decided to only get the cumulative row (e.g., Both Sexes), drop the other rows that represent the division (e.g., Female and Male), and drop the column that is related to this division (Sex). \n",
    "\n",
    "Fifth, we convert the DataFrame into its long representation. Once we have the dataset into its long representation, then we can merge it to the combined dataset while using the Year and Geolocation columns as its primary key. This would be done for all of the twenty-five datasets.\n",
    "\n",
    "This process would result in one DataFrame that is in its long representation, with three kinds of columns: (1) Geolocation, (2) Year, and (3) the value for each of the indicators. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39340350",
   "metadata": {},
   "source": [
    "#### 1.2.1. Proportion of population living below the national poverty line \n",
    "To start with, let us load the data from the csv file using pandas' [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function.\n",
    "\n",
    "The [`os.getenv`](https://docs.python.org/3/library/os.html) function was used to get the environment variable `DSDATA_PROJ`, which points to the data folder of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14568c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/1.2.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/1.2.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b658bd",
   "metadata": {},
   "source": [
    "Looking at the DataFrame, we could see that the columns are unnamed and that the column names are located at the 0th row. Using [`iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html), we could get the 0th row and then assign it as the column values. \n",
    "\n",
    "Then, using the [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) function, we can drop the 0th row as we have no need for it anymore. Additionally, since the row at index 1 is a row full of NaN, we can also drop it using the same function. \n",
    "\n",
    "To be able to fix the indexing of the rows, the [`reset_index`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) function was used to reset the index from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae001ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting our column names\n",
    "data.columns = data.iloc [0] \n",
    "\n",
    "# dropping the 'geolocation' row as that is actually used as a header\n",
    "data = data.drop (data.index [1])\n",
    "\n",
    "# dropping the column names \n",
    "data = data.drop (data.index [0])\n",
    "\n",
    "data.reset_index (drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79485cc1",
   "metadata": {},
   "source": [
    "Irrelevant rows that are just footers for the file are also removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8bc10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant rows \n",
    "data = data.drop (data.index [18:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebebaa06",
   "metadata": {},
   "source": [
    "The `Year` column must also be renamed into `Geolocation` as this row refers to the different regions in the Philippines, and not the years. This can be done through the use of the of the [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5184c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the column 'Year' as its actually the location column\n",
    "data.rename(columns = {'Year':'Geolocation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d0c896",
   "metadata": {},
   "source": [
    "To easily determine which region the `Geolocation` values refer to, we can also change these values to include the names that they are commonly referred to, instead of just their region numbers. \n",
    "\n",
    "For consistency throughout the different datasets, the `region_names` variable was declared. The reason why a map was not used was that different datasets have different representations of the region (i.e., differences in naming a region), however, they are always arranged in the same way. This would be shown below in the pre-processing of each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Before applying, make sure that the arrangement of the regions are the same as the arrangement in your table\n",
    "region_names = ['PHILIPPINES', 'NCR: National Capital Region', \n",
    "                 'CAR: Cordillera Administrative Region', \n",
    "                 'Region 1: Ilocos Region', \n",
    "                 'Region 2: Cagayan Valley', \n",
    "                 'Region 3: Central Luzon', \n",
    "                 'Region 4A: CALABARZON', \n",
    "                'MIMAROPA: Southwestern Tagalog Region', \n",
    "                'Region 5: Bicol Region', \n",
    "                'Region 6: Western Visayas', \n",
    "                'Region 7: Central Visayas', \n",
    "                'Region 8: Eastern Visayas', \n",
    "                'Region 9: Zamboanga Peninsula', \n",
    "                'Region 10: Northern Mindanao', \n",
    "                'Region 11: Davao Region', \n",
    "                'Region 12: SOCCSKSARGEN', \n",
    "                'CARAGA: Cordillera Administrative Region', \n",
    "                'BARMM: Bangsamoro Autonomous Region in Muslim Mindanao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95da29a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Geolocation'] = region_names\n",
    "data.set_index('Geolocation')\n",
    "data = data.reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2153b",
   "metadata": {},
   "source": [
    "Next, we can convert the strings of '..' and '...', which were used to represent that there were no values for these cells, to **NaN**, through the use of the [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) function.\n",
    "\n",
    "However, the columns that have all **NaN** values were not dropped because if this dataset would be combined with other datasets, all years would still be present as there are datasets with complete data for all the years. Additionally, dropping the years for some of the dataset would result in the combined dataset having a weird sorting (i.e., a sorting of the region that does not follow the usual sorting of the datasets in the Philippines), even if it was sorted based on the `Year` and `Geolocation` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce63be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    # cells without values are represented as either '..' or '...', so we should convert them to NaN so we could dropna()\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# drops columns if all of the values are NaN\n",
    "# data = data.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971705fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c523309",
   "metadata": {},
   "source": [
    "As the final step, the wide representation of this dataset is converted to a long representation through the use of the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function. \n",
    "\n",
    "Then, the column that holds the value for a specific year and region is coverted, using [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html), to the ID of this Sustainable Development Goal (SDG), so that it can be distinguished when it is combined with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372cde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'1.2.1. Proportion of population living below the national poverty line', 0 : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20284161",
   "metadata": {},
   "source": [
    "As this is the first dataset, we can just assign it to the `combined_data` DataFrame, which would hold the combined datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798efeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca9864",
   "metadata": {},
   "source": [
    "#### 1.4.1p5. Net Enrolment Rate in elementary\n",
    "\n",
    "Using the same [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function, we load the next dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f265d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/1.4.1p5.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/1.4.1p5.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd020ae5",
   "metadata": {},
   "source": [
    "From the DataFrame above, we can see that the footer of the .csv files was included in the DataFrame. As the rows from the 56th index are irrelevant, we can [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (data.index [56:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce36258",
   "metadata": {},
   "source": [
    "Additionally, we can see that the columns are unnamed, and upon inspection, the original column names can be found at `Index 0`. Thus, we can set the columns to this row, and then  [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the `Index 0` row as it would only be redundant and might affect the computations.\n",
    "\n",
    "The [`reset_index`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) function was used in order to make the index of the rows start from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cddbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the column names and removing the row that held the previous column names\n",
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9007900e",
   "metadata": {},
   "source": [
    "However, these is still a row of NaN found at `Index 0`, and we can see that the column names for the first two columns are not correct for the values underneath it, as the ones under the first column are actually Geolocations and those under the second columns are the values for Sex. Thus, we can [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) it, and then [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)  the row at `Index 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231defb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns = {np.nan:'Geolocation', 'Year': 'Sex'})\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d6fbf",
   "metadata": {},
   "source": [
    "As we would only need the data that is grouped by region and not by sex, we would only be getting the rows that has **Both Sexes** as the value in the Sex column. After this, we can [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the Sex column as it would not be used onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c99d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only getting the total data, then dropping Sex column as it's not needed anymore\n",
    "data = data[data['Sex'] == 'Both Sexes']\n",
    "data = data.drop(\"Sex\", axis = 1)\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5019ce",
   "metadata": {},
   "source": [
    "To be able to merge this to the combined DataFrame, the value of the Geolocation column has been set to the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdfb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1217f7e",
   "metadata": {},
   "source": [
    "Since the dataset represents missing values as either '...' or '..', we can [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) the columns with these values with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42240c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    # cells without values are represented as either '..' or '...', so we should convert them to NaN so we could dropna()\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53f166",
   "metadata": {},
   "source": [
    "Then, we can transform the wide representation of the DataFrame to its long representation version using the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'1.4.1p5 Net Enrolment Rate in elementary', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd4ddcc",
   "metadata": {},
   "source": [
    "Then we can [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) this long representation DataFrame into the combined DataFrame. It would be merged with respect to the values in the **Geolocation** and **Year** column. An outer join is used as we want to retain all the values of both of the DataFrames, even if there would be **NaN** values for some of cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b615c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c476809",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9171bad",
   "metadata": {},
   "source": [
    "#### 1.4.1p6. Net Enrolment Rate in secondary education (Indicator is also found in SDG 4.3.s2)\n",
    "\n",
    "Next, we can load the third dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc03bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/1.4.1p6.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/1.4.1p6.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c4063",
   "metadata": {},
   "source": [
    "Just like in the processing of the previous datasets, we first [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the unnecessary rows at the bottom part of the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161badf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (data.index [110:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc3c24",
   "metadata": {},
   "source": [
    "From the DataFrame above, we can see that the correct column headers are found at `Index 0`. However, upon inspection, we would see that there are two NaN values and the 'Year' value at the third column should actually be 'Sex' based on the values below it. Thus, before setting this row as the column header, we first correct the values of these first three columns using the [`at`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0, '1.4.1p6 Net Enrolment Rate in secondary education (Indicator is also found in SDG 4.3.s2)'] = 'Level of Education'\n",
    "data.at[0, 'Unnamed: 1'] = 'Geolocation'\n",
    "data.at[0, 'Unnamed: 2'] = 'Sex'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c18347",
   "metadata": {},
   "source": [
    "Now that first row can correctly act as the column header, we can set is as the column header, before dropping the row at `Index 0`. Then we must also [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the row of **NaN**s at `Index 1` as it is unnecessary, before using the [`reset_index`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd410f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0876e",
   "metadata": {},
   "source": [
    "Using the [`unique`](https://pandas.pydata.org/docs/reference/api/pandas.unique.html) function, we can see that there are two values for 'Level of Education' columns. To be able to combine this to the combined dataset, we must separate them as we cannot add another column that would hold the education level, thus, we can just add it as two different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Level of Education'].unique ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96be64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_data = data [54:]\n",
    "junior_high_data = data [:54]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c98c9b5",
   "metadata": {},
   "source": [
    "Now, we must process these two separately, but the processes done to them would be the same.\n",
    "\n",
    "First, as we only need the general data, without taking *Sex* into consideration. This can be done by only getting the rows that has **Both Sexes** as the value of the `Sex` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_high_data = junior_high_data [junior_high_data['Sex'] == 'Both Sexes']\n",
    "junior_high_data = junior_high_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_data = senior_high_data [senior_high_data['Sex'] == 'Both Sexes']\n",
    "senior_high_data = senior_high_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc1934-b819-4802-a81e-e3c6d4f3483c",
   "metadata": {},
   "source": [
    "Next, as we have already separated the dataset into two based on the value of the `Level of Education` column, we have no need for this column anymore. This means that we can [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) this column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f57eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_high_data = junior_high_data.drop(\"Level of Education\", axis = 1)\n",
    "junior_high_data = junior_high_data.drop(\"Sex\", axis = 1)\n",
    "junior_high_data = junior_high_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a100777",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_data = senior_high_data.drop(\"Level of Education\", axis = 1)\n",
    "senior_high_data = senior_high_data.drop(\"Sex\", axis = 1)\n",
    "senior_high_data = senior_high_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d6d32-8c2b-4bf3-925a-b9281614a412",
   "metadata": {},
   "source": [
    "For consistency, we set the values of the `Geolocation` column to the format of the region names that we have decided before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_data['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_high_data['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d82bf3-74bd-47a4-a2a1-01acc440200e",
   "metadata": {},
   "source": [
    "As the dataset represents missing values as '..' or '...', we must [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.Series.replace.html) these values with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in junior_high_data.columns.difference(['Geolocation']):\n",
    "    junior_high_data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    junior_high_data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b4c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in senior_high_data.columns.difference(['Geolocation']):\n",
    "    senior_high_data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    senior_high_data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e0ccd1-fe39-4114-8565-c04122ef45c4",
   "metadata": {},
   "source": [
    "Looking at the senior high data, we can see that all of the values are `NaN` from 2000 to 2016, which is to be expected as Senior High School was only implemented from 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2e75e-2cb5-41fe-b2e0-04fb9cc251ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f67ba3-7375-48c7-afe8-a689a5b4c1f9",
   "metadata": {},
   "source": [
    "Next, we can convert both of the datasets into its long representation using the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33932bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_high_data = pd.melt(junior_high_data, id_vars='Geolocation', value_vars=junior_high_data.columns [1:]) \n",
    "\n",
    "junior_high_data.rename(columns = {'value':'1.4.1p6 Net Enrolment Rate in secondary education (Junior High School)', 0 : 'Year'}, inplace=True)\n",
    "junior_high_data = junior_high_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c5df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_data = pd.melt(senior_high_data, id_vars='Geolocation', value_vars=senior_high_data.columns [1:]) \n",
    "\n",
    "senior_high_data.rename(columns = {'value':'1.4.1p6 Net Enrolment Rate in secondary education (Senior High School)', 0 : 'Year'}, inplace=True)\n",
    "senior_high_data = senior_high_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f187a10-c3c1-4f9c-b984-d4e2b1b64a2e",
   "metadata": {},
   "source": [
    "Once that both datasets has been converted to their long representation, we can [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) the two datasets to the combined dataset based on the values of the `Geolocation` and the `Year` column with an outer join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(junior_high_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.merge(senior_high_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ed15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c643a5d",
   "metadata": {},
   "source": [
    "#### 1.5.4. Proportion of local governments that adopt and implement local disaster risk reduction strategies in line with national disaster risk reduction strategies\n",
    "Then, the fourth dataset could be loaded using the same [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58698f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/1.5.4.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/1.5.4.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca9665-aecc-4cba-a237-662f2602e81e",
   "metadata": {},
   "source": [
    "Same as the previous datasets, we would need to [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the irrelevant rows at the bottom of the DataFrame. These are the rows that were a footer outside of the table in the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190f147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop (data.index [19:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e7501-60e7-4b44-bdd4-d74c096bd20b",
   "metadata": {},
   "source": [
    "Likewise, we know that the row at `Index 0` has the values that is the supposed column header for the table. However, checking each of the cells in this row would make us realize that the column header for the first column should not be `Year`, but rather `Geolocation` as the values in these columns refer to the different regions. \n",
    "\n",
    "Thus, we can change the value of the first column in this row to `Geolocation`, so that we would not need to rename the column if we directly made the 0th row into the column header. Then, we can [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the row at `Index 0` as it is now unnecessary. Additionally, we can see that there is a row of **NaN**s at `Index 1`, which would become the 0th row once we drop the row that became the column headers. This should be dropped also, before the index is resetted using the [`reset_index`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c399bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0, '1.5.4 Proportion of local governments that adopt and implement local disaster risk reduction strategies in line with national disaster risk reduction strategies (Indicator can also found in SDG 13.1.3 and 11.b.2)'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63de6eb-c487-4023-ab5c-4b3551521446",
   "metadata": {},
   "source": [
    "The next step would be renaming the values under the `Geolocation`, although, as seen in the resulting table, we would notice that there is no row for **PHILIPPINES**. This is reflected in the way that we set the values of this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names [1:]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3d59e-8daf-459d-b55c-f8fecae9590b",
   "metadata": {},
   "source": [
    "As with the previous datasets, we would have to [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) the '..' and '...' values, which represents **null**, in the DataFrame with **NaN**s. This is to avoid any errors that would happen in these rows, and so that it would be represented properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3442e1-8620-4306-9c3d-5fccb0a373e4",
   "metadata": {},
   "source": [
    "After all of this, we can now transform this dataset that is in its wide represetation into its long representation using the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b770d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb0881-2638-42af-87f5-189005ec7ce4",
   "metadata": {},
   "source": [
    "Once we were able to convert it to its long representation, we would see that the column names in this new DataFrame are not descriptive with respect to the values underneath the column. Directly merging this with the combined DataFrame would make it hard for its users to distinguish what these columns are for, which is why it was [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)d to its correct column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2082e-c04a-42dd-a39d-85ad84e52db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'value':'1.5.4 Proportion of local governments that adopt and implement local disaster risk reduction strategies in line with national disaster risk reduction strategies', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f7139c-f337-445e-b8fd-27fb77486948",
   "metadata": {},
   "source": [
    "After this, we can now [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) it to the combined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc2c1d",
   "metadata": {},
   "source": [
    "#### 3.4.1. Mortality rate attributed to cardiovascular disease, cancer, diabetes or chronic respiratory disease\n",
    "To start with the fifth dataset, let us load the data from the csv file using pandas' [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1742b038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/3.4.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/3.4.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ae9cb-9d71-4002-9e76-d59718a0e57c",
   "metadata": {},
   "source": [
    "Based on the DataFrame that we got using the [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function, we can see that there are rows of **NaN**s at the lower part of the DataFrame. Upon further inspection, it started from `Index 266`, which is why the rows from this index was [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)ped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f41d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (data.index [266:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34e299-bf3a-4c7a-b1b1-ef5a53dd4ee9",
   "metadata": {},
   "source": [
    "As the column headers are all **Unnamed**, we need to set the column headers to its correct value, which is found at `Index 0`. Although, the values for the first three columns in this row are not descriptive to be column headers, which is why we are changing their values to the correct descriptive name for the rows underneath them using the [`at`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at.html) function.\n",
    "\n",
    "As we have no use for the row at `Index 0`, we can now [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) this row. With this, we would also be [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)ping the next row as it is just a row of **NaN**s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe28044",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0, '3.4.1 Mortality rate attributed to cardiovascular disease, cancer, diabetes or chronic respiratory disease'] = 'Indicator'\n",
    "data.at[0, 'Unnamed: 1'] = 'Geolocation'\n",
    "data.at[0, 'Unnamed: 2'] = 'Sex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92317517",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbc3d1-ef9e-4db6-a7fd-9ec3202680db",
   "metadata": {},
   "source": [
    "As the `Sex` column is not available for all datasets, it was decided that only the total—or those rows with **Both Sexes**—would be considered. Once we our data only includes rows with **Both Sexes** as the value of their `Sex` column, we can now [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) this column as this column would only have one unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b50ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data [data ['Sex'] == 'Both Sexes']\n",
    "data = data.drop('Sex', axis = 1)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceded53-ad50-4010-8247-5b882a6fbae7",
   "metadata": {},
   "source": [
    "Then, we need to [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) all cells that has the value of either '..' or '...' with **NaN** for better computation in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbdff86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b108e45-2b94-4be4-bc5b-b05a693b6dd8",
   "metadata": {},
   "source": [
    "Upon studying the different indicators under this specific Sustainable Development Goal (SDG), we would realize that it is comprised of different subsets: (1) cardiovascular diseases, (2) cancer, (3) diabetes, and (4) chronic respiratory disease. However, as we only aim to get the total mortality rate with respect to all of these diseases, we would only get the rows under this indicator which is from `Index 0` to `Index 16`.\n",
    "\n",
    "Then, after dividing the different subsets, we can now [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the `Indicator` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00315903-8819-4d5a-ac06-c1d1bd44a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Indicator'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254a530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = data [0:16]\n",
    "cardio_data = data [16:34]\n",
    "cancer_data = data [34:52]\n",
    "diabetes_data = data [52:70]\n",
    "respi_data = data [70:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop('Indicator', axis = 1)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1dd47-89af-4fba-b545-9c7dd95531c4",
   "metadata": {},
   "source": [
    "Upon inspection, we would realize that there are two regions that are missing from the table, which are **Region V** and **Region VI**, which is why we would only be using the region names that are included in the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no region five and six\n",
    "all_data ['Geolocation'] = region_names [0:8] + region_names [10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c8c88-559a-4805-90d4-8fede47929b9",
   "metadata": {},
   "source": [
    "After this, with the use of the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function, we can now convert our DataFrame to its long representation. Then, we must set the column headers to describe the values in this column, which is why we would need to [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72077d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.melt(all_data, id_vars='Geolocation', value_vars=all_data.columns [1:]) \n",
    "\n",
    "all_data.rename(columns = {'value':'3.4.1 Mortality rate attributed to cardiovascular disease, cancer, diabetes or chronic respiratory disease (Total data)', 0 : 'Year'}, inplace=True)\n",
    "all_data = all_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca8fe0a-89a8-444e-9a13-a17e4602d971",
   "metadata": {},
   "source": [
    "After this, we can now [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) it to the DataFrame which holds the combined datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(all_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7f027",
   "metadata": {},
   "source": [
    "#### 3.7.1. Proportion of women of reproductive age (aged 15-49 years) who have their need for family planning satisfied [provided] with modern methods\n",
    "\n",
    "Using the same [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function, we load the sixth dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c272de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/3.7.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/3.7.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79779e9",
   "metadata": {},
   "source": [
    "Irrelevant rows that are just footers for the file are also [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)ped. From the DataFrame above, we can see that these are the rows from `Index 20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d094b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (data.index [20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec07ab96",
   "metadata": {},
   "source": [
    "Additionally, we can see that the current column names are **Unnamed**. Thus, we have to set the column names to its correct values so that we can determine what the values in the columns are.\n",
    "\n",
    "Understanding the data, we can see that the row at `Index 0` holds the value for the column headers. However, there is a **NaN** value, which should be **Geolocation** based on the data underneath it. This is why the value of this cell was changed to **Geolocation** using the [`at`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at.html) function.\n",
    "\n",
    "This is done before the column names was set to the row at `Index 0`, and then [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)ping this row and the row of NaNs at the next row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a31a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0, 'Unnamed: 1'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c980c",
   "metadata": {},
   "source": [
    "Added to this, we can see that there is a column of **NaN**s, which we do not need, so we can also [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34dd347",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('Year', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd2d4c",
   "metadata": {},
   "source": [
    "Just like what we have done in the previous datasets, we would rename the **Geolocation** column based on the common names of the region for easier understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237483ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06ab1c8",
   "metadata": {},
   "source": [
    "As the missing data or null values in the dataset are represented by '..' or '...', which are strings that might affect the computations that might be done in this numerical columns, we would be using the [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) function to replace these string values to **np.nan**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5d466",
   "metadata": {},
   "source": [
    "As the dataset now looks like the wide representation that we wanted, we would be transforming it to its long representation, using the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function, so that we could merge it to the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d22f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d37f7e",
   "metadata": {},
   "source": [
    "Although, before merging it to the combined dataset, we would need to [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) the columns `0` and `value`, as they are not descriptive enough. If we directly merged it to the combined dataset, we might not be able to determine what the values in these columns mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'value':'3.7.1 Proportion of women of reproductive age (aged 15-49 years) who have their need for family planning satisfied [provided] with modern methods', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1712d",
   "metadata": {},
   "source": [
    "Once the column names have been fixed, we could use the [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) function to use outer join to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ec856",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcba981",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eff4b9",
   "metadata": {},
   "source": [
    "#### 3.7.2. Adolescent birth rate aged 15-19 years per 1,000 women in that age group\n",
    "Then, the seventh dataset could be loaded using the same [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4093d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/3.7.2.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/3.7.2.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b320b",
   "metadata": {},
   "source": [
    "As seen in the previous datasets, there are three types of columns that are processed and [`drop`]ped first: (1) the irrelevant rows that were footers in the .csv file, (2) the row that would be turned into the column headers, and (3) the row of **NaN**s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (data.index [20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d15492",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f033c",
   "metadata": {},
   "source": [
    "Although, we can see that there is a column name that does not correctly represent the data of this column: the `Year` column does not indicate years, but rather the regions. This is why it was [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)d to `Geolocation`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e03309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'Year':'Geolocation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a9fbb",
   "metadata": {},
   "source": [
    "Once we have cleaned the column headers, the values for the `Geolocation` column would be fixed to include their common names. It is important to note that it was made sure that each of the row completely match the arrangement in the `region_name` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40460ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e1a98",
   "metadata": {},
   "source": [
    "As we now have fixed the number of rows and the column names, we would now replace the string representation of null or missing vlaues. This is done with the use of [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) function, which would convert the '..' and '...' values into **np.nan**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ec64b4",
   "metadata": {},
   "source": [
    "Then, we can now convert our DataFrame into its long representation using the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function. As in the processing of the previous datasets, we would have to [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) the column names as they are not descriptive enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'3.7.2 Adolescent birth rate aged 15-19 years per 1,000 women in that age group', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638434c",
   "metadata": {},
   "source": [
    "As we are now sure that the missing or null values are correctly represented, the values of the `Geolocation` are now more easily understandable, and the column headers are descriptive enough, we can now merge this dataset into the combined datasets using the [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d1f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b834b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7175b9",
   "metadata": {},
   "source": [
    "#### 4.1.s1. Completion Rate of elementary and secondary students\n",
    "To start with the eighth dataset, let us load the data from the csv file using pandas' [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d2a3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/4.1.s1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/4.1.s1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bb5de",
   "metadata": {},
   "source": [
    "From the view of the DataFrame above, we can see that there are unnecessary rows captured by the  [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function. To be able to correctly represent the data, we would need to [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[164:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a15c4",
   "metadata": {},
   "source": [
    "Another problem that we have based on the DataFrame shown above is the lack of column names, as shown in the **Unnamed** values in the header. Studying the DataFrame, we would find the supposed column headers in the row of `Index 0`, though we face the problem of having **NaN** values at the first three columns of this row. This is why the values in these cells are changed using the [`at`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at.html) function, before converting this row to be the column header.\n",
    "\n",
    "After we have been able to turn this into the column header, we would need to drop this row and the row beneath it as they are unnecessary rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab049343",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0, '4.1.s1 Completion Rate of elementary and secondary students 1/ 2/'] = 'Geolocation'\n",
    "data.at[0, 'Unnamed: 1'] = 'Level of Education'\n",
    "data.at[0, 'Unnamed: 2'] = 'Sex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82609555-68d2-4e68-baa2-baf0ed21d707",
   "metadata": {},
   "source": [
    "Just like in datasets that has the `Sex` column, we would only be getting rows with the value for this column as **Both Sexes**. Afterwards, as we already have no need for this column anymore, we can [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data [data['Sex'] == 'Both Sexes']\n",
    "data = data.drop ('Sex', axis = 1)\n",
    "data = data.reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca917d6-45d2-4173-860e-aa719e6a7cdf",
   "metadata": {},
   "source": [
    "As we can see from the resulting dataset, there are still **NaN** values in the `Geolocation` column, which we do not want as this would be used in merging the datasets together. However, if we study it, we would realize that the reason for this is that one value for `Geolocation` actually spans to the next two rows as there are different values for the `Level of Education` column. Although, we cannot just separate the dataset per unique value of the `Level of Education` column, as the `Geolocation` would be NaN for all  **Secondary (Junior High School)** and **Secondary (Senior High School)**. \n",
    "\n",
    "Due to this, we copy the value of the `Geolocation` column of a row to the next two rows after it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the geolocation value to the next two rows\n",
    "i = 0\n",
    "while i < len (data):\n",
    "    if i % 3 == 0:\n",
    "        data.at[i + 1, 'Geolocation'] = data['Geolocation'][i]\n",
    "        data.at[i + 2, 'Geolocation'] = data['Geolocation'][i]\n",
    "        i = i + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416b09e-0657-4c54-8eab-dc7c4717fbd2",
   "metadata": {},
   "source": [
    "Before we divide the dataset based on the value of `Level of Education`, we must first replace cells with the strings '..' or '...' with **np.nan**. This is so that we would not need to process this representation of missing or null values separately (i.e., per division). Then, we can now separate them so that we can properly label it before merging it to the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd134861",
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_data = data [data['Level of Education'] == 'Elementary']\n",
    "elem_data = elem_data.reset_index (drop=True)\n",
    "\n",
    "junior_data = data [data['Level of Education'] == 'Secondary (Junior High School)']\n",
    "junior_data = junior_data.reset_index (drop=True)\n",
    "\n",
    "senior_data = data [data['Level of Education'] == 'Secondary (Senior High School)']\n",
    "senior_data = senior_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90255f5-add8-4dc4-9579-4b734b55fd82",
   "metadata": {},
   "source": [
    "Once we have successfully divided the dataset based on the value of the `Level of Education` column, we can now [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) this column as each of the division would technically only have one value for this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_data = elem_data.drop ('Level of Education', axis = 1)\n",
    "elem_data = elem_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_data = junior_data.drop ('Level of Education', axis = 1)\n",
    "junior_data = junior_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_data = senior_data.drop ('Level of Education', axis = 1)\n",
    "senior_data = senior_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f064c7-ce57-4dd7-9e1d-7f34eeff831e",
   "metadata": {},
   "source": [
    "After making sure that the arrangement of the region matches the arrangement of the values of the `region_names` variable, we can change the values of the `Geolocation` column for each of the division. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca0b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c33ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673be70d-9e0d-4288-b018-7c83961b5d0a",
   "metadata": {},
   "source": [
    "Then, we can now convert the DataFrames into their long representation, before using the [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) function to make the column names more descriptive of the data in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5de434",
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_data = pd.melt(elem_data, id_vars='Geolocation', value_vars=elem_data.columns [1:]) \n",
    "\n",
    "elem_data.rename(columns = {'value':'4.1.s1 Completion Rate of elementary and secondary students (Elementary)', 0 : 'Year'}, inplace=True)\n",
    "elem_data = elem_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_data = pd.melt(junior_data, id_vars='Geolocation', value_vars=junior_data.columns [1:]) \n",
    "\n",
    "junior_data.rename(columns = {'value':'4.1.s1 Completion Rate of elementary and secondary students (Junior High School)', 0 : 'Year'}, inplace=True)\n",
    "junior_data = junior_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67c323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "senior_data = pd.melt(senior_data, id_vars='Geolocation', value_vars=senior_data.columns [1:]) \n",
    "\n",
    "senior_data.rename(columns = {'value':'4.1.s1 Completion Rate of elementary and secondary students (Senior High School)', 0 : 'Year'}, inplace=True)\n",
    "senior_data = senior_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1dce58-3100-4866-abda-b4f3dd58c559",
   "metadata": {},
   "source": [
    "As we have now made sure that each of division would be understandable even if combined with the combined dataset, we can now [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) each of them into the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(elem_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(junior_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c673f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(senior_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4a9b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dbdc54",
   "metadata": {},
   "source": [
    "#### 4.c.s2. Number of Technical-Vocational Education and Training (TVET) trainers trained\n",
    "Next, we can load the ninth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa557a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/4.c.s2.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/4.c.s2.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c76129-8a09-4c42-bc88-1e6b740cded3",
   "metadata": {},
   "source": [
    "As usual, we would first be [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)ping the irrelevant rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf021a0-4bf3-430e-9a69-2718b508c3dc",
   "metadata": {},
   "source": [
    "Then, as we know that the correct column headers are found at `Index 0`, we have to fix the values of this row to fully represent the data in the columns. This is why the **Year** value was changed into **Geolocation** because the values in this column are the rows of the country.\n",
    "\n",
    "After this, we can now make the value of this row as the value of the column headers, before [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)ping this row as it would not be used anymore. In line with this, we can also [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) the row of **NaN**s underneath this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0, '4.c.s2 Number of Technical-Vocational Education and Training (TVET) trainers trained'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77dfadf-283a-4564-8d32-e918493e86e8",
   "metadata": {},
   "source": [
    "Then, we need to change the values of the `Geolocation` column to match the prescribed format for the region names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04dce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f7e2f1-19e8-4c4c-a5fe-d86b6683ca5c",
   "metadata": {},
   "source": [
    "After this, we need to clean the dataset by turning the string representation of missing or null values, which are '..' and '...', into **np.nan**. This would allow us to correctly use mathematical functions into these columns without errors arising due to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f000334",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)\n",
    "\n",
    "# data = data.dropna(axis=1, how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458f1b9-b0ad-4f8f-ab48-f01a13c3b89a",
   "metadata": {},
   "source": [
    "Once we have done this, we can convert the DataFrame into its long representation, which would allow us to merge it with the combined dataset. Converting a DataFrame that is in its wide representation into its long representation is made possible by the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function.\n",
    "\n",
    "However, using the [`melt`](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) function would result into a three-column DataFrame which has the following column names: (1) `Geolocation`, (2) `0`, and (3) `value`. The last two columns are not properly descriptive of the values of the column, which is why these two columns are [`rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)d. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'4.c.s2 Number of Technical-Vocational Education and Training (TVET) trainers trained', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08405c65-013f-4e05-a9c1-c5a0650319f9",
   "metadata": {},
   "source": [
    "As we now have a DataFrame that is in its long representation, we can now [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) it to the combined DataFrame, with respect to the values of the `Geolocation` and `Year` columns. This means that a row from this DataFrame would be [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html)d into the combined dataset on the row that has the same `Geolocation` and `Year`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49135c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f509cc-0456-44aa-a64d-965cc909cd4d",
   "metadata": {},
   "source": [
    "#### 7.1.1. Proportion of population with access to electricity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e5d01-9f61-4dfd-af21-600f7c2f8700",
   "metadata": {},
   "source": [
    "Now, we will proceed to loading the tenth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac594cf-64f2-48bc-a6af-ad8ae14619b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/7.1.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/7.1.1.csv') // AJ TO DO\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776c44f-e095-4d65-b9d5-1d408318e7c2",
   "metadata": {},
   "source": [
    "Before anything else, we drop the irrelevant rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518a796-142c-4d45-ae75-d3538d27525e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop(data.index[20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad23de-e28d-4daf-be1a-19572513b55e",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we will change the data in Index 0 at column '7.1.1 Proportion of population with access to electricity 1/' into 'Geolocation' since our goal is to make the geolocation the first column of the dataframe. By doing this, Index 0 has now the correct column headers. \n",
    "\n",
    "With this, we have to arrange the values of this row to fully represent the data in the columns. Therefeore, we will now make the value of this row as the value of the column headers. \n",
    "\n",
    "After this, we drop this row (Index 0) as it would not be used anymore as well as the row of NaNs underneath this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120e492-b2f2-473a-b146-ed735e1a0314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.at[0,'7.1.1 Proportion of population with access to electricity 1/'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74492c5f-0979-4394-9ba9-aef718f97d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b247c55-d8b1-47f7-a3c9-28e653b6ddb9",
   "metadata": {},
   "source": [
    "After checking if the order of the Geolocation is the same as what we intended, we will initialize the Geolocation column of the region names to make sure that the format of the region names in this dataset is the same as the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34f233-20bb-42f4-b634-eb6595a227a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2475af-6a5f-4eca-9629-03f000144826",
   "metadata": {},
   "source": [
    "We will then change the the '..' or '...' strings to NaN using the np.nan. Again, these NaN values were not dropped because all years from 2001-2022 will be in the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025dea61-9107-4683-b2bd-12d016ef480f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b1cdca-8f36-4d2d-afb4-7abb0fa64fa7",
   "metadata": {},
   "source": [
    "We can now convert the DataFrame into its long representation using the `melt` function. This would allow us to merge it with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6bee18-27b1-4d60-a5af-4e3c90ab024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'7.1.1 Proportion of population with access to electricity', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8b9ad-444e-4fc3-bdaa-bbfc8c3be189",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a99ba1-4579-4fa8-9855-8facc65293c8",
   "metadata": {},
   "source": [
    "We will now combine this dataset to the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b73f9-ce13-4da4-bd99-1550a052db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeed33a-43fc-46f4-b325-030535f6e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc1cc6-45e6-4a8f-8187-9b033e95b49e",
   "metadata": {},
   "source": [
    "#### 8.1.1. Annual growth rate of real GDP per capita"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983eba7-a81a-40d2-8691-fe5567cab1aa",
   "metadata": {},
   "source": [
    "Loading the eleventh dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dde51a-d476-4fa9-bc07-8433f48227b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/8.1.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/8.1.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304fa29-87d1-40ae-89dd-98930bbc54f4",
   "metadata": {},
   "source": [
    "We will now drop the irrelevant rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ee564-13e6-4787-952b-d47fea3320b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cdb8c-7c92-49ac-a36b-32cd1ae6052e",
   "metadata": {},
   "source": [
    "Observing the header column and the Index 0, the data in the Index 0 is much more similar to the column names we want for the dataset. With this, it would be more hassle to change all columns names in the header column than changing the data in Index 0 and setting it to be the header column.\n",
    "\n",
    "With this, we will change the data in at Index 0 Column 0 into 'Geolocation' since our goal is to make the geolocation the first column of the dataframe. By doing this, Index 0 has now the correct column headers. Then, will now make the value of this row as the value of the column headers.\n",
    "\n",
    "After this, we drop this row (Index 0) as it would not be used anymore as well as the row of NaNs underneath this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa413ad-5af9-4cc1-80ab-7ec19f626c19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.at[0,'8.1.1 Annual growth rate of real GDP per capita'] = 'Geolocation'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597ac77-0cb8-4d3a-8167-b914275c971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f928e9-962c-476b-9397-4c2381400a93",
   "metadata": {},
   "source": [
    "After checking if the order of the Geolocation is the same as what we intended, we will initialize the Geolocation column of the region names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d656e1d-e29a-4d5d-aa6e-154ba3cae441",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05eda14-c8ba-4ab7-8406-0a1e65fa6e40",
   "metadata": {},
   "source": [
    "To represent the missing values clearly, we change the the '..' or '...' strings to NaN using the np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82542b29-d9eb-4c9e-9b9c-1e3f431126ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35a0c5-c768-49eb-a45f-fd677db1fc5f",
   "metadata": {},
   "source": [
    "We can now convert the DataFrame into its long representation to allow us to merge it with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe63cf-da54-446a-a186-896e6eb300e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'8.1.1 Annual growth rate of real GDP per capita', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaad9fc-8f5b-46c1-a331-bb4711e16f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85581d2-5d44-4eff-81dc-2462969c695c",
   "metadata": {},
   "source": [
    "After this, we combine this dataset with the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae274e88-d544-436f-a8fa-433eb3c4ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c93d29-937a-42ef-89df-fc47c857ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975d192-f4fb-4af6-a9dc-506543515668",
   "metadata": {},
   "source": [
    "#### 10.1.1. Growth rates of household expenditure or income per capita among the bottom 40 per cent of the population and the total population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d487c550-6aa4-4971-915c-dcc2653f87dc",
   "metadata": {},
   "source": [
    "Loading the twelfth dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26902028-d532-4665-b9dc-0f2defadfa08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/10.1.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/10.1.1.csv') \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12158073-ea7a-4ec3-87e5-3410c1dd8491",
   "metadata": {},
   "source": [
    "Dropping the irrelevant rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883527c-4d33-4d2f-a864-bcb14412a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[38:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd7767-fa5b-4f10-a7ef-fd827a7d7ea6",
   "metadata": {},
   "source": [
    "We will change the data in at Index 0 Column 0 into 'Geolocation' since our goal is to make the geolocation the first column of the dataframe. By doing this, Index 0 has now the correct column headers. Then, will now make the value of this row as the value of the column headers.\n",
    "\n",
    "After this, we drop this row (Index 0) as it would not be used anymore as well as the row of NaNs underneath this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e8e53b-c32a-4e25-9907-6af9a1a8b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0,'10.1.1 Growth rates of household expenditure or income per capita among the bottom 40 per cent of the population and the total population'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a581be-c6fd-49ff-bab4-92abb818c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ee2a7-6792-413d-83eb-923b4e3e4500",
   "metadata": {},
   "source": [
    "To represent the missing values clearly, we change the the '..' or '...' strings to NaN using the np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b103940-b966-401a-b64f-4ff55b04c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db70c3-e15a-4143-9bf9-331048a01822",
   "metadata": {},
   "source": [
    "As observed in this dataset, we have two parts which are **10.1.1.1 Bottom 40 percent of the population** and **10.1.1.2 Total Population**. Since we will both need these parts, we will still get both parts to combine with other datasets. However, we will divide them into two different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9ca5d-0bc4-40ff-9a48-a4bde69eeb45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['Geolocation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759f09c-470b-4dac-a72a-9c3a1c54e338",
   "metadata": {},
   "source": [
    "**10.1.1.1 Bottom 40 percent of the population** goes to `bottom_popu_data` while **10.1.1.2 Total Population** goes to `total_popu_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1a8d9-6807-4e99-9844-2287e9c7a7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bottom_popu_data = data [0:18]\n",
    "total_popu_data = data [18:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375860bd-46fb-4b21-aa2c-57ad57aac83d",
   "metadata": {},
   "source": [
    "Since `total_popu_data` started with index 18, we will set its starting index to 0. \n",
    "\n",
    "Also, since the first row of each of the parts is a record for the Philippines and the order of the geolocation of each DataFrame is correct, we will initialize it with the region_names for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ace7bc-5c95-419a-adc7-1e8902bda937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_popu_data = total_popu_data.reset_index (drop=True)\n",
    "\n",
    "bottom_popu_data ['Geolocation'] = region_names\n",
    "total_popu_data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a79861-9fc0-4a7c-99d4-2d3d7742ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_popu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1d929-f5f9-4f30-b00f-cc6e72ef4158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_popu_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86675795-19df-4b59-bfe6-cff9301e01a4",
   "metadata": {},
   "source": [
    "We can now convert both DataFrames into their long representation to allow us to merge both of them with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946d5f2-b838-4c4b-b0fe-d3bf8be0e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_popu_data = pd.melt(bottom_popu_data, id_vars='Geolocation', value_vars=bottom_popu_data.columns [1:]) \n",
    "\n",
    "bottom_popu_data.rename(columns = {'value':'10.1.1.1 Bottom 40 percent of the population', 0 : 'Year'}, inplace=True)\n",
    "bottom_popu_data = bottom_popu_data.astype({'Year':'int'})\n",
    "\n",
    "total_popu_data = pd.melt(total_popu_data, id_vars='Geolocation', value_vars=total_popu_data.columns [1:]) \n",
    "\n",
    "total_popu_data.rename(columns = {'value':'10.1.1.2 Total Population', 0 : 'Year'}, inplace=True)\n",
    "total_popu_data = total_popu_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5a1e8-1be6-4cbc-9b34-1869a5b66be0",
   "metadata": {},
   "source": [
    "Combining the two different datasets with the currently combined data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c34890-dde5-4f91-892e-2c5b76c836ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the 10.1.1.1 dataset with the current combined dataset\n",
    "combined_data = combined_data.merge(bottom_popu_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)\n",
    "# Adding the 10.1.1.2 dataset with the current combined dataset\n",
    "combined_data = combined_data.merge(total_popu_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4663fb12-d5ec-4d2d-85cc-c9601cc120b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f512849-a954-4168-90e7-1286086a1ed4",
   "metadata": {},
   "source": [
    "#### 14.5.1. Coverage of protected areas in relation to marine areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603e52b-0774-49a2-b504-4626e5db4834",
   "metadata": {},
   "source": [
    "We will now read the thirteenth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426094d2-f3bb-425c-ac17-0fecb4b318b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/14.5.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/14.5.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19dd3b5-894f-4c18-8cab-3cd81e81332d",
   "metadata": {
    "tags": []
   },
   "source": [
    "As usual, we drop the irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0718d0-5866-452e-a12c-9ec881a87959",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop (data.index [38:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bc2d75-6f57-44c4-bfdc-03f263717c03",
   "metadata": {},
   "source": [
    "We will now edit the data in column 0 & 1 at Index 0 to make the whole Index 0 look like the column headers we want.Then, we set the Index 0 to become the header columns. After this, we drop the Index 0 and the row of NaNs underneath it.\n",
    "\n",
    "The first column was renamed to `Indicator` as it contains the names of the parts in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11feeaf-d55a-4ddf-8448-5ce99e6b6957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.at[0, '14.5.1 Coverage of protected areas in relation to marine areas'] = 'Indicator'\n",
    "data.at[0, 'Unnamed: 1'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba711b8-684a-4a21-b299-62bc0e9fa2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3493583d-2836-4826-9f62-2c8b56e20236",
   "metadata": {},
   "source": [
    "To represent the missing values clearly, we change the the '..' or '...' strings to NaN using the np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ea91f-6dbd-46c6-9e23-7dd99dcbf73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2c553-81b9-4a60-b6c8-93d2be67e3e4",
   "metadata": {},
   "source": [
    "As observed in this dataset, we have two parts which are **14.5.1.1 Coverage of protected areas in relation to marine areas, Universe (in million hectares)** and **14.5.1.2 Coverage of protected areas in relation to marine areas, NIPAS ans Locally managed MPAs 1/**. Since we will both need these parts, we will still get both parts to combine with other datasets. However, we will divide them into two different datasets.\n",
    "\n",
    "For this, we will retain the `Indicator` column first, which contains the name of the parts, for identifying how this dataset will be divided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dad953-4e3f-4533-9c9b-2b23be581288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['Indicator'].unique()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad04ca-c19a-4deb-b227-0dfa8fcbcf5f",
   "metadata": {},
   "source": [
    "**14.5.1.1 Coverage of protected areas in relation to marine areas, Universe (in million hectares)** goes to `universe_data` while **14.5.1.2 Coverage of protected areas in relation to marine areas, NIPAS ans Locally managed MPAs 1/** goes to `nipas_data`. \n",
    "\n",
    "Since the `nipas_data` will start at Index 18, we will reset it to Index 0 after the division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34971e-5d5b-4450-bf0f-67d1c3ab227d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "universe_data = data [0:18]\n",
    "nipas_data = data [18:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d43d91-c8a0-4d15-b345-94d4609b833f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nipas_data = nipas_data.reset_index (drop=True)\n",
    "nipas_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d7933-8e69-4052-8306-868165c1a434",
   "metadata": {},
   "source": [
    "Since the dividing of the dataset is done, we won't be needing the `Indicator` column anymore. Therefore, we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d8dc9-4cc3-4a55-a3f9-0f8784400cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "universe_data = universe_data.drop('Indicator', axis = 1)\n",
    "nipas_data = nipas_data.drop('Indicator', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f5fbad-3be1-4aa9-90ef-f6e9b9348f99",
   "metadata": {
    "tags": []
   },
   "source": [
    "Since the order of the geolocation of each DataFrame is correct, we will initialize it with the region_names for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e3577-b5d4-49f5-82d6-518eea3f1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_data ['Geolocation'] = region_names\n",
    "nipas_data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0c8095-939a-44a0-9707-d80a22863d1a",
   "metadata": {},
   "source": [
    "We can now convert both DataFrames into their long representation to allow us to merge both of them with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c34de-32cd-4a4b-ab02-71f1e2147914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 14.5.1.1\n",
    "universe_data = pd.melt(universe_data, id_vars='Geolocation', value_vars=universe_data.columns [1:]) \n",
    "universe_data.rename(columns = {'value':'14.5.1.1 Coverage of protected areas in relation to marine areas, Universe (in million hectares)', 0 : 'Year'}, inplace=True)\n",
    "universe_data = universe_data.astype({'Year':'int'})\n",
    "# 14.5.1.2\n",
    "nipas_data = pd.melt(nipas_data, id_vars='Geolocation', value_vars=nipas_data.columns [1:]) \n",
    "nipas_data.rename(columns = {'value':'14.5.1.2 Coverage of protected areas in relation to marine areas, NIPAS ans Locally managed MPAs', 0 : 'Year'}, inplace=True)\n",
    "nipas_data = nipas_data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82564de9-cd17-47ca-a37c-960b1dff7ef2",
   "metadata": {},
   "source": [
    "Combining the two different datasets with the currently combined data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80436a8b-6f4e-4400-9f83-29dd57810434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding the 14.5.1.1 dataset with the current combined dataset\n",
    "combined_data = combined_data.merge(universe_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)\n",
    "# Adding the 14.5.1.2 dataset with the current combined dataset\n",
    "combined_data = combined_data.merge(nipas_data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418dec9b-2732-4eb4-8afd-716405423184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77579d29-6c7f-4573-8898-45bb1bebce13",
   "metadata": {},
   "source": [
    "#### 16.1.1 Number of victims of intentional homicide (per 100,000 population)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4987f6f-acbf-44eb-b20d-555b7eae1311",
   "metadata": {},
   "source": [
    "We will now load the fourteenth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9901d8-d514-47b2-b0b1-8075b5083806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/16.1.1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/16.1.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d69677-496b-4ef3-985f-08e49413c2a4",
   "metadata": {},
   "source": [
    "Now, we will drop the irrelevant rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb31169-67ec-4a42-b55c-74fae3453f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ea885-94d6-4015-b88a-b90e41fa3b5b",
   "metadata": {},
   "source": [
    "Since Index 0 is almost the same as the column header we want, we will just change the content in the first column to `Geolocation`. This also because the column already containes the regions of the Philippines.\n",
    "\n",
    "Then, we set the Index 0 to become the header column. After this, we will drop Index 0 and the rows of NANs underneath it since we will not be needing this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac413ccf-99b3-4e74-a839-80f4e0c153a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0,'16.1.1 Number of victims of intentional homicide (per 100,000 population) 1/'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420d36b-a678-428b-ae2a-dc80a16ab4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1459fdd-ac78-4c70-83bc-c7604a8ea1f6",
   "metadata": {},
   "source": [
    "We will now check the order of the Geolocation if it is the same as the combined dataset. Then, to make the naming of Geolocation uniformed, we will initialized the Geolocation with region_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41edc841-eb4b-443b-8b7e-a58f935cd3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3a4e3-ce4f-4e34-b1f4-be8049693f54",
   "metadata": {},
   "source": [
    "We will then change the the '..' or '...' strings to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe43386-eec0-4102-b921-6a3593fa7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3690d43-65d2-4886-be8c-2250d2d0fb81",
   "metadata": {},
   "source": [
    "We can now convert the DataFrame into its long representation using the melt function. This would allow us to merge it with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8a630-0148-45ce-becf-351cf55cc557",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'16.1.1 Number of victims of intentional homicide (per 100,000 population)', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac79f82-6dca-4263-a702-04a8a1ca1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52428452-d34c-4433-a171-651a1f3bc293",
   "metadata": {},
   "source": [
    "We will now combine this dataset to the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2f358-24f1-41b5-a710-118f2ee019da",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe202a-11d3-4c3a-b885-d75ba1b24b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f65a94-cafe-4eee-8529-8cbc684a9632",
   "metadata": {},
   "source": [
    "#### 16.1.s1 Number of murder cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec4dbf-3dc2-4966-baa6-c37857f85f5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We are now loading our fifteenth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454e609-1640-4ba5-b679-97bd2245f2d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/16.1.s1.csv')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/16.1.s1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016856f1-52c4-4f29-91ce-1fdba303f576",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "Dropping the irrelevant rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc8faf9-b0e0-4d4e-a7ca-3f198e317402",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccd718-7ba4-4400-bdd9-283901ee3d98",
   "metadata": {},
   "source": [
    "Since Index 0 is almost the same as the column header we want, we will just change the content in the first column to \"Geolocation\". Then, we set the Index 0 to become the header column. After this, we will drop Index 0 and the rows of NANs underneath it since we will not be needing this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ef585-e5b6-4698-b8bd-15f6c8e64d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[0,'16.1.s1 Number of murder cases'] = 'Geolocation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b37aab-f004-417a-8af2-77e49bfd77ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.columns = data.loc[0]\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "\n",
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93eec99-02a3-4a9d-a552-51d5c08bc52f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We will now check the order of the Geolocation if it is the same as the combined dataset. Then, to make the naming of Geolocation uniformed, we will initialized the Geolocation with region_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc8a75-03a5-4706-b16d-faa82ee18528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ad2a0-3246-48f0-8c8c-6e779c207319",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will then change the the '..' or '...' strings to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f42646-86ca-4fce-a42a-d1813be5b61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793827cf-f942-4d72-86c8-baeb451bb81d",
   "metadata": {},
   "source": [
    "We can now convert the DataFrame into its long representation using the melt function. This would allow us to merge it with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b195c05-23e7-499a-b8cc-d59b1100bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "data.rename(columns = {'value':'16.1.s1 Number of murder cases', 0 : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b7890-3618-4dec-b9f5-90598f2402b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac97fc49-acb0-4569-baf7-bd7a4dfa9a09",
   "metadata": {},
   "source": [
    "We will now combine this dataset to the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46166faa-1900-4cee-b13d-3e38ee490b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7a52c-79b3-46b8-93a1-331408ffa0c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732fc967-f688-4a87-a209-a8229eb7413c",
   "metadata": {},
   "source": [
    "#### Other Non-SDG datasets\n",
    "These are datasets that can provide us with more context when exploring the datasets for the Sustainable Development Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1a934-e3a8-458a-92bb-3519160ca439",
   "metadata": {},
   "source": [
    "##### Changes in Inventories, by Region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80f6924-8201-414c-ad25-0c03722726dd",
   "metadata": {},
   "source": [
    "Loading the sixteenth dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90053c8b-4c3f-466f-9f11-316ea402784b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Changes in Inventories, by Region.csv', header=1, delimiter=\";\")\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/Changes in Inventories, by Region.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef9662-1ded-4d14-90ee-507aec90f359",
   "metadata": {},
   "source": [
    "Since we will be only needng the current prices and we will not be focusing on comparing each record to 2018, we will be dropping the columns with `At Constant 2018 Prices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff48bc2-a571-40c3-987b-274d548ee598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.drop(data.iloc[:, 22:], inplace=True, axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed8feb-b9dc-4f37-be62-328e7d4cbf5d",
   "metadata": {},
   "source": [
    "Also, since the ordering of the Geolocation is different in this dataset, we will be rearranging the rows based on the order of the Geolocation in `region_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1669da-fb7d-4f6c-a273-3bb76f48d387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.reindex(index=[17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6784ecd-5c56-4f4b-b63b-ec773985137b",
   "metadata": {},
   "source": [
    "Then, we will proceed to reindexing the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb721bd2-759e-465c-8fa7-d0ba0b99421f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286dae38-1769-45e0-9b21-f761f80c9c71",
   "metadata": {},
   "source": [
    "After this, we will now change the columns names: (1) `Region` to `Geolocation`, (2) `At Current Prices <Year>` to `<Year>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4cf13-ce10-42b1-ac32-8bc5be6d801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['Geolocation', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008',\n",
    "               '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017','2018', '2019', '2020']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12902b83-578f-4956-bf37-e686752e7bbb",
   "metadata": {},
   "source": [
    "After this, we will insert the region_names in the Geolocation column so that the format of the region_names will fit the ones in the combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37411f8-997e-40cf-97ac-5c20a846d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc6a429-2b2e-4ead-9640-6df40931b76e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "We will then change the the '..' or '...' strings to NaN. This is to represent the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636783f9-1af4-4adf-8699-cd0264a4b823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac773e5a-d917-4e62-8ab6-ec2140660d21",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We can now convert the DataFrame into its long representation using the melt function. This would allow us to merge it with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695827e1-ce5a-462b-9708-539afad1ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "data.rename(columns = {'value':'Changes in Inventories, by Region', 'variable' : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af16dd3-e89c-4e7f-b6ef-d1929274d316",
   "metadata": {},
   "source": [
    "Finally, we will now combine this dataset to the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c88b0-1cd9-427f-913f-2763d7663760",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c0452-9a4e-4d63-bb90-0a978531bcc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c3fc1-2059-4ec9-a9b1-df3b1f2ad860",
   "metadata": {},
   "source": [
    "##### Current Health Expenditure by Region, Growth Rates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db8f0",
   "metadata": {},
   "source": [
    "Loading the seventeenth dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d8048-ca97-4fa6-ab15-caefd531ba28",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Current Health Expenditure by Region, Growth Rates.csv', header=1, delimiter=\";\")\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/Current Health Expenditure by Region, Growth Rates.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f41b7-98fe-4a07-95d9-e798111c718b",
   "metadata": {},
   "source": [
    "Since we will only need the data nationwide and per region, we will drop the `Index 0` which contains the Total Current Health Expenditure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4475650-d691-4080-8c60-aff9302a7647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop (data.index[0])\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d722f5-9f2c-45c5-bec2-f1ab692c5f78",
   "metadata": {},
   "source": [
    "Also, since the ordering of the Geolocation is different in this dataset, we will be rearranging the rows based on the order of the Geolocation in region_names. After this, we will reset the index again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47acbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.reindex(index=[17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])\n",
    "data = data.reset_index (drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af9c27-732d-4868-b220-53ff78b6bd32",
   "metadata": {},
   "source": [
    "After this, we will now change the columns names: (1) `Region` to `Geolocation`, (2) `At Current Prices <Year>` to `<Year>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a0522-15e7-439e-a39c-63e84ee298cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.columns = ['Geolocation', '2014', '2015', '2016', '2017','2018', '2019']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac5748-1f40-491b-9b64-890068a24293",
   "metadata": {},
   "source": [
    "As observed, this dataset does not have the records for the years: 2000-2013 and 2020-2022. To allow this dataset to merge with the currently combined dataset easily, we add additional columns for representing the missing years in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab1c31-d28a-45b0-b651-183bd486fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding columns 2000-2013\n",
    "col = 1\n",
    "for i in range(2000,2014):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1\n",
    "# For adding columns 2020-2022\n",
    "col = 21\n",
    "for i in range(2020,2023):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88579f-20ea-4bd3-8516-c92f975af7dd",
   "metadata": {},
   "source": [
    "After this, we will insert the region_names in the Geolocation column so that the format of the region_names will fit the ones in the combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54f802-2186-4fd1-8506-f7cb81d93887",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ['Geolocation'] = region_names\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b993a-1094-4118-9379-32656a4635d6",
   "metadata": {},
   "source": [
    "We will then change the the '..' or '...' strings to NaN. This is to represent the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2b84b-c1f8-45a9-a560-18d51e64368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns.difference(['Geolocation']):\n",
    "    data [c].replace(to_replace='..', value= np.nan, inplace= True)\n",
    "    data [c].replace(to_replace='...', value= np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337fc47-f540-46da-baef-2d5b82732db8",
   "metadata": {},
   "source": [
    "We can now convert the DataFrame into its long representation using the melt function. This would allow us to merge it with the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870d20d-881e-43b0-b3c9-36a8e8f2b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "data.rename(columns = {'value':'Current Health Expenditure by Region, Growth Rates', 'variable' : 'Year'}, inplace=True)\n",
    "data = data.astype({'Year':'int'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5320f21b-ead0-4334-aae4-34c8d920c421",
   "metadata": {},
   "source": [
    "Finally, we will now combine this dataset to the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c334912-6276-44ca-9e76-d4292838e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a0a0e-5a5d-472b-85ad-282f92321931",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2731e92-dcb5-40bc-98ad-991054dd8da4",
   "metadata": {},
   "source": [
    "##### Current Health Expenditure by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a44fe-c168-42ec-bfc2-3bc6a71428db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Current Health Expenditure by Region.csv',header = 1,sep = ';')\n",
    "# data = pd.read_csv(os.getenv('DSDATA_PROJ') + '/Current Health Expenditure by Region.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop total current health expenditure\n",
    "data = data.drop (data.index[0])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b25ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#remove '..' and 'r'\n",
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data.columns = data.columns.str.replace('[r]', '',regex = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4baf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make nationwide index 0\n",
    "data = data.iloc[np.arange(-1, len(data)-1)]\n",
    "data = data.reset_index()\n",
    "data.drop('index', axis = 1,inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afdfae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9bf88-f81d-4a76-b9bb-0cacbc32ed06",
   "metadata": {},
   "source": [
    "To follow the format of the combined dataset and to make combining dataset easier, we add columns for years: `2000-2013` and `2020-2022`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf130287-4ac0-4162-929d-46d30cce9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding columns 2000-2013\n",
    "col = 1\n",
    "for i in range(2000,2014):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1\n",
    "# For adding columns 2021 and 2022\n",
    "data.insert(22, 2021, np.nan, True)\n",
    "data.insert(23, 2022, np.nan, True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb90a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'Current Health Expenditure by Region', 'variable' : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9173cc-5b50-4903-bfe6-1e38da48a362",
   "metadata": {},
   "source": [
    "Finally, we will now add this dataset with the currently combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136cca4-5e61-4497-b5f2-077dfb33290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff71659-5570-41c7-bfc6-55297c52375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d6a4d-d8d0-4198-ba6f-1eb208f19326",
   "metadata": {},
   "source": [
    "##### Government Final Consumption Expenditure, by Region, Growth Rates\n",
    "Load next dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec5697-b9b1-4a3d-805d-c9104f5e312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Government Final Consumption Expenditure, by Region, Growth Rates.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938bc5f",
   "metadata": {},
   "source": [
    "We remove the '..' at the start of the Region column values then put the last row as the first row to follow the format of region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove '..' and arrange row\n",
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data = data.iloc[np.arange(-1, len(data)-1)]\n",
    "data = data.reset_index()\n",
    "data.drop('index', axis = 1,inplace = True)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dcce35",
   "metadata": {},
   "source": [
    "We rename the Region column to region_names for consistency then rename the column header Region to Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62aea5",
   "metadata": {},
   "source": [
    "We only need the at current price for that year so we drop  the not needed columns. We then format the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32be0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.iloc[:, 21:41], inplace = True, axis = 1)\n",
    "data.columns = data.columns.map(lambda x: x.lstrip('At Current Prices'))\n",
    "data.columns = data.columns.str[:4]\n",
    "data.rename(columns = {'Geol': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75271cd",
   "metadata": {},
   "source": [
    "Add missing columns 2020-2022 to be able to merge easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding columns 2020-2022\n",
    "col = 21\n",
    "for i in range(2020,2023):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d3e43",
   "metadata": {},
   "source": [
    "Then, we can now convert our DataFrame into its long representation using the melt function. As in the processing of the previous datasets, we would have to rename the column names as they are not descriptive enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'Consumption Expenditure GR', 'variable' : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b353e3a",
   "metadata": {},
   "source": [
    "We use the merge function to use outer join to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1d698-f683-4d98-9bb7-835dcfec9f52",
   "metadata": {},
   "source": [
    "##### Government Final Consumption Expenditure, by Region, Percent Share\n",
    "Load the next dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaae661-4fa9-4c34-af40-ce4aa383700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Government Final Consumption Expenditure, by Region, Percent Share.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff2d3e5",
   "metadata": {},
   "source": [
    "We remove the '..' at the start of the Region column values then put the last row as the first row to follow the format of region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813dec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove '..' and arrange row\n",
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data = data.iloc[np.arange(-1, len(data)-1)]\n",
    "data = data.reset_index()\n",
    "data.drop('index', axis = 1,inplace = True)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfad970",
   "metadata": {},
   "source": [
    "We rename the Region column to region_names for consistency then rename the column header Region to Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d59821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f136c3",
   "metadata": {},
   "source": [
    "We only need the at current price for that year so we drop  the not needed columns. We then format the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.iloc[:, 22:43], inplace = True, axis = 1)\n",
    "data.columns = data.columns.map(lambda x: x.lstrip('At Current Prices'))\n",
    "data.columns = data.columns.str[:4]\n",
    "data.rename(columns = {'Geol': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7f605",
   "metadata": {},
   "source": [
    "Add missing columns 2021-2022 to be able to merge easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding columns 2021-2022\n",
    "col = 22\n",
    "for i in range(2021,2023):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7af62",
   "metadata": {},
   "source": [
    "Then, we can now convert our DataFrame into its long representation using the melt function. As in the processing of the previous datasets, we would have to rename the column names as they are not descriptive enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfcd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'Consumption Expenditure %', 'variable' : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0af19",
   "metadata": {},
   "source": [
    "We use the merge function to use outer join to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f38b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0985036",
   "metadata": {},
   "source": [
    "##### Gross Capital Formation, by Region\n",
    "Load the next dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b97f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Gross Capital Formation, by Region.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccea7c9",
   "metadata": {},
   "source": [
    "We remove the '..' at the start of the Region column values then put the last row as the first row to follow the format of region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data = data.iloc[np.arange(-1, len(data)-1)]\n",
    "data = data.reset_index()\n",
    "data.drop('index', axis = 1,inplace = True)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dade0f",
   "metadata": {},
   "source": [
    "We rename the Region column to region_names for consistency then rename the column header Region to Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640f0c5",
   "metadata": {},
   "source": [
    "We only need the at current price for that year so we drop  the not needed columns. We then format the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c86c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.iloc[:, 22:43], inplace = True, axis = 1)\n",
    "data.columns = data.columns.map(lambda x: x.lstrip('At Current Prices'))\n",
    "data.columns = data.columns.str[:4]\n",
    "data.rename(columns = {'Geol': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9abe575",
   "metadata": {},
   "source": [
    "Add missing columns 2021-2022 to be able to merge easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding columns 2021-2022\n",
    "col = 22\n",
    "for i in range(2021,2023):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983bba4",
   "metadata": {},
   "source": [
    "Then, we can now convert our DataFrame into its long representation using the melt function. As in the processing of the previous datasets, we would have to rename the column names as they are not descriptive enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'Gross Capital Formation', 'variable' : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67564873",
   "metadata": {},
   "source": [
    "We use the merge function to use outer join to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a40c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7c798",
   "metadata": {},
   "source": [
    "##### Gross Regional Domestic Product, by Region\n",
    "Load the next dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ba214",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Gross Regional Domestic Product, by Region.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03e90c",
   "metadata": {},
   "source": [
    "We remove the '..' at the start of the Region column values then put the last row as the first row to follow the format of region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f066543",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data = data.iloc[np.arange(-1, len(data)-1)]\n",
    "data = data.reset_index()\n",
    "data.drop('index', axis = 1,inplace = True)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa700d4",
   "metadata": {},
   "source": [
    "We rename the Region column to region_names for consistency then rename the column header Region to Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb38537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f5a1d",
   "metadata": {},
   "source": [
    "We only need the at current price for that year so we drop  the not needed columns. We then format the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb291d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.iloc[:, 22:43], inplace = True, axis = 1)\n",
    "data.columns = data.columns.map(lambda x: x.lstrip('At Current Prices'))\n",
    "data.columns = data.columns.str[:4]\n",
    "data.rename(columns = {'Geol': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2400e",
   "metadata": {},
   "source": [
    "Add missing columns 2021-2022 to be able to merge easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding columns 2021-2022\n",
    "col = 22\n",
    "for i in range(2021,2023):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c44ed",
   "metadata": {},
   "source": [
    "Then, we can now convert our DataFrame into its long representation using the melt function. As in the processing of the previous datasets, we would have to rename the column names as they are not descriptive enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46419ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'GRDP', 'variable' : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffbdbe8",
   "metadata": {},
   "source": [
    "We use the merge function to use outer join to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96552b55-efd9-441e-8200-ebb9928dd645",
   "metadata": {},
   "source": [
    "##### Population, by Region\n",
    "Load the next dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68896e-e099-4579-b990-5dadf0dc0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Population, by Region.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99930ee",
   "metadata": {},
   "source": [
    "We remove the '..' at the start of the Region column values then put the last row as the first row to follow the format of region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0443f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data = data.iloc[np.arange(-1, len(data)-1)]\n",
    "data = data.reset_index()\n",
    "data.drop('index', axis = 1,inplace = True)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae7f45",
   "metadata": {},
   "source": [
    "We rename the Region column to region_names for consistency then rename the column header Region to Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422bf50",
   "metadata": {},
   "source": [
    "Add missing columns 2021-2022 to be able to merge easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding columns 2021-2022\n",
    "col = 22\n",
    "for i in range(2021,2023):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e68ef7",
   "metadata": {},
   "source": [
    "Then, we can now convert our DataFrame into its long representation using the melt function. As in the processing of the previous datasets, we would have to rename the column names as they are not descriptive enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25bb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'Population', 'variable' : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4527a99",
   "metadata": {},
   "source": [
    "We use the merge function to use outer join to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7efb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc4fab-a350-40ac-8c9b-8e969d201340",
   "metadata": {},
   "source": [
    "##### Primary Drop-out rates by Region, Sex and Year\n",
    "Load next dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e6627-1069-4329-982e-6dfff30e4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Primary Drop-out rates by Region, Sex and Year.csv',header = 1,sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02b474",
   "metadata": {},
   "source": [
    "We rename the Region column to region_names for consistency then rename the column header Region to Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d90849",
   "metadata": {},
   "source": [
    "Drop the unnecessary columns as we only need the data for both sexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1441d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.iloc[:, 11:31], inplace = True, axis = 1)\n",
    "  \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4018044",
   "metadata": {},
   "source": [
    "Strip the 'Both Sexes' at the start of the column name so only the year would remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f000325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.map(lambda x: x.lstrip('Both Sexes '))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a757f72",
   "metadata": {},
   "source": [
    "Add missing columns to be able to merge easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding columns 2000-2005\n",
    "col = 1\n",
    "for i in range(2000,2006):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1\n",
    "    \n",
    "# For adding columns 2016-2022\n",
    "col = 17\n",
    "for i in range(2016,2023):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b3f19c",
   "metadata": {},
   "source": [
    "Then, we can now convert our DataFrame into its long representation using the melt function. As in the processing of the previous datasets, we would have to rename the column names as they are not descriptive enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [1:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'Drop-out rate', 'variable' : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa1b44",
   "metadata": {},
   "source": [
    "We use the merge function to use outer join to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efeea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6addc98-c67d-4dc8-917c-16bf20d9ac60",
   "metadata": {},
   "source": [
    "##### Quarterly Producer Price Index for Agriculture (First Quarter 2018 to Third Quarter 2021)\n",
    "Load the next dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b8a54-63cc-497e-8f27-b79dd95555dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data' + '/Quarterly Producer Price Index for Agriculture (2018=100) _ First Quarter 2018 to Third Quarter 2021.csv',header = 1,sep = ';')\n",
    "data[data['Commodity'] == 'AGRICULTURE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136119a9",
   "metadata": {},
   "source": [
    "We remove the '..' at the start of the Region column values then put the last row as the first row to follow the format of region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff80306",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Region'] = data['Region'].map(lambda x: x.lstrip('..'))\n",
    "data['Commodity'] = data['Commodity'].map(lambda x: x.lstrip('..'))\n",
    "data['Commodity'] = data['Commodity'].map(lambda x: x.lstrip('….'))\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd7d76",
   "metadata": {},
   "source": [
    "Since there is no NCR in this dataset we declare region_names again this time without NCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a856e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Before applying, make sure that the arrangement of the regions are the same as the arrangement in your table\n",
    "region_names1 = ['PHILIPPINES', \n",
    "                 'CAR: Cordillera Administrative Region', \n",
    "                 'Region 1: Ilocos Region', \n",
    "                 'Region 2: Cagayan Valley', \n",
    "                 'Region 3: Central Luzon', \n",
    "                 'Region 4A: CALABARZON', \n",
    "                'MIMAROPA: Southwestern Tagalog Region', \n",
    "                'Region 5: Bicol Region', \n",
    "                'Region 6: Western Visayas', \n",
    "                'Region 7: Central Visayas', \n",
    "                'Region 8: Eastern Visayas', \n",
    "                'Region 9: Zamboanga Peninsula', \n",
    "                'Region 10: Northern Mindanao', \n",
    "                'Region 11: Davao Region', \n",
    "                'Region 12: SOCCSKSARGEN', \n",
    "                'CARAGA: Cordillera Administrative Region', \n",
    "                'BARMM: Bangsamoro Autonomous Region in Muslim Mindanao']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59739647",
   "metadata": {},
   "source": [
    "We only take the Agriculture then we rename the Region column to region_names for consistency then rename the column header Region to Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Commodity'] == 'AGRICULTURE']\n",
    "\n",
    "# renames the data in the Geolocation for consistency\n",
    "data['Region'] = region_names1\n",
    "data.set_index('Region')\n",
    "data = data.reset_index(drop=True)\n",
    "data.rename(columns = {'Region': 'Geolocation'},inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e0794a",
   "metadata": {},
   "source": [
    "We keep the column that has the average for that year and drop the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347879c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Geolocation','Commodity','2018 Average (Jan-Dec)','2019 Average (Jan-Dec)','2020 Average (Jan-Dec)']]\n",
    "data.columns = data.columns.str[:4]\n",
    "data.rename(columns = {'Geol': 'Geolocation','Comm':'Commodity' },inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f725d70",
   "metadata": {},
   "source": [
    "Add missing columns to be able to merge easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding columns 2000-2017\n",
    "col = 2\n",
    "for i in range(2000,2018):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1\n",
    "# For adding columns 2021-2022\n",
    "col = 23\n",
    "for i in range(2021,2023):\n",
    "    data.insert(col, str(i), np.nan, True)\n",
    "    col+=1\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c646d",
   "metadata": {},
   "source": [
    "Then, we can now convert our DataFrame into its long representation using the melt function. As in the processing of the previous datasets, we would have to rename the column names as they are not descriptive enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39bf806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from a wide representation to a long representation\n",
    "data = pd.melt(data, id_vars='Geolocation', value_vars=data.columns [2:]) \n",
    "\n",
    "# renaming the columns into a more readable anmes\n",
    "data.rename(columns = {'value':'Price Index for Agriculture', 'variable' : 'Year'}, inplace=True)\n",
    "\n",
    "# making the year type into integer\n",
    "data = data.astype({'Year':'int'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d03cb",
   "metadata": {},
   "source": [
    "We use the merge function to use outer join to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f096bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.merge(data, how = 'outer', on = ['Geolocation', 'Year'])\n",
    "combined_data = combined_data.reset_index (drop=True)\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598039d-dd32-4c86-a557-f9154f9170f2",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "There are four steps for the cleaning of the combined dataset: (1) the dropping of the rows wherein all the values of the indicator columns are **NaN**s, (2) the fixing of the data types of the columns, (3) the dropping of duplicated rows, and (4) the cleaning of the individual columns.\n",
    "\n",
    "### Dropping of rows that has all **NaN** values\n",
    "The first thing that we would do is to drop the rows that only have **NaN** values. This means that for that specific region in that specific year, there is no data that is collected for any of the indicators, thus, we would not be able to derive any knowledge from it.\n",
    "\n",
    "Using the combination of the isna and sum functions, we would be able to see the total number of **NaN** values a specific row has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37508d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_data.isna().sum(axis = 1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83b6bd",
   "metadata": {},
   "source": [
    "From the result above, we can see that there are rows that have all **NaN** values (i.e., where the number of **NaN** values outputted is equal to the number of the columns for indicators). Since we know that the `Geolocation` and `Year` column does not have any **NaN** values, we would set a threshold of 3 (which means that if there are at least three non-NaN values, the row would not be dropped ) in the [`dropna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.dropna(axis = 0, thresh = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda2c07e",
   "metadata": {},
   "source": [
    "With this, we would have a new DataFrame that has 377 rows, with the `Year` having a range of from 2000 to 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a50f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['Year'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd81d1",
   "metadata": {},
   "source": [
    "### Fixing the Data Types of the Columns\n",
    "Using the [`dtypes`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html) property, we would see that some indicators columns are **object**-types. As we know that all columns except for the `Geolocation` and `Year` are supposed to be **float64** columns, we would need to convert these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b448f",
   "metadata": {},
   "source": [
    "For each of the column that are not the `Geolocation` and `Year` columns, their datatypes are checker. In the scenario that they are not **float64**, the function [`astype`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) was used in order to convert it to float. Even though we are sure that all of the values in these columns can be transformed to float as this was its original value in the csv file, the parameter `errors` was still set to **raise** for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93148661",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in combined_data.columns.difference(['Geolocation', 'Year']):\n",
    "    if combined_data[x].dtypes != 'float64':\n",
    "        combined_data.loc[:, x] = combined_data[x].astype(float, errors = 'raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53b28f",
   "metadata": {},
   "source": [
    "Using the [`info`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) function, we would see that all the indicator columns are now **float64**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e169396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad1e9d",
   "metadata": {},
   "source": [
    "### Dropping of Duplicated Rows\n",
    "Using a combination of [`duplicated`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html) and [`sum`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html), we would be able to see how many rows are duplicated and should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd0598",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01f873",
   "metadata": {},
   "source": [
    "As the combination of these functions outputted the number 0, then we can conclude that each of the rows are unique. This means that we would not have to drop any of the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1283769",
   "metadata": {},
   "source": [
    "### Cleaning of Each Columns\n",
    "As each of the column came from different datasets, we would be checking and cleaning the values for each of the column.\n",
    "\n",
    "#### 1.2.1. Proportion of population living below the national poverty line\n",
    "For this column, we would be using the [`describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) function in order to check if we have an outliner. This is due to the fact that we are expecting a value of 0 to 10, as we are talking about proportion or percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['1.2.1. Proportion of population living below the national poverty line'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde3781",
   "metadata": {},
   "source": [
    "From what we can see, the minimum and maximum values of the columns are within the range of values that we expected from this column. Thus, there are no outliers that we need to remove or drop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f8524",
   "metadata": {},
   "source": [
    "#### 1.4.1p5 Net Enrolment Rate in elementary\n",
    "Just like in the first column, we would be using the [`describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) function in order to check the value range of the variable. \n",
    "\n",
    "According to the Philippine Statistics Authority (n.d.), the formula for net enrollment rate in elementary is defined as total enrollment of aged six to 11, divided by the population of kids of the same age, and then multiplied by 100. For this, we are expecting a value of 0 to 100, as we are talking about a percentage of a population: we cannot have more children enrolled than the total population of kids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f21f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data['1.4.1p5 Net Enrolment Rate in elementary'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d388c9",
   "metadata": {},
   "source": [
    "As we can see, the maximum value of this column is higher than 100, which can be concerning as the unit of measurement set by United Nations for all of the countries in this indicators is percentage. Thus, these might be error in encodings.\n",
    "\n",
    "Let us check all of the rows which has values higher than 100 for this indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data[combined_data['1.4.1p5 Net Enrolment Rate in elementary'] > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d65c5a0",
   "metadata": {},
   "source": [
    "As we can see, there are 18 rows which has more than 100% value for the `1.4.1p5 Net Enrolment Rate in elementary`. In order to prevent these values from skewing the data in the scenario that it is used for computation, these values are instead turned into **NaN**s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ff944",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.loc[combined_data['1.4.1p5 Net Enrolment Rate in elementary'] > 100, '1.4.1p5 Net Enrolment Rate in elementary'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2be150",
   "metadata": {},
   "source": [
    "Now, we can see that all of the values for this column are now within the range that we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca59adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data[combined_data['1.4.1p5 Net Enrolment Rate in elementary'] > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786392ed",
   "metadata": {},
   "source": [
    "#### 1.4.1p6 Net Enrolment Rate in secondary education (Junior High School)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a6771",
   "metadata": {},
   "source": [
    "As we have the same expectations in the second dataset, the [`describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) function would be used in order to check if there are outliers or values that are outside of the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['1.4.1p6 Net Enrolment Rate in secondary education (Junior High School)'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08582c3f",
   "metadata": {},
   "source": [
    "From the minimum and maximum value, we can see that the range of values are within the expected values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32225f71",
   "metadata": {},
   "source": [
    "#### 1.4.1p6 Net Enrolment Rate in secondary education (Senior High School)\n",
    "Next, in this column, we would be using the [`describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) function for the same purpose: checking if the maximum and minimum values are within the range we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac38d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data['1.4.1p6 Net Enrolment Rate in secondary education (Senior High School)'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21c8f4",
   "metadata": {},
   "source": [
    "Based on the output, we can see that the minimum and maximum are within the range.\n",
    "\n",
    "However, another expectation that we have from this column is that the rows that are not **NaN** have a value of **2016 - onwards** for the `Year` column. This is due to the fact that the Senior High School years was only added from 2016. Thus, if there are values for years lower than this, we would need to turn it to **NaN**.\n",
    "\n",
    "To check this, we can use a mixture of the [`isnull`](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html) function and the [`unique`](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html) function. Using the negation of the [`isnull`](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html) function, we can only return rows that are not missing. Then, using the [`unique`](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html) function, we can return the unique values of the `Year` column of the previously returned rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data[~combined_data['1.4.1p6 Net Enrolment Rate in secondary education (Senior High School)'].isnull()]['Year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7c3b9",
   "metadata": {},
   "source": [
    "We can see that the values of the `Year` column of the rows that are not **NaN** for this column are what we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc4c69e",
   "metadata": {},
   "source": [
    "#### 1.5.4 Proportion of local governments that adopt and implement local disaster risk reduction strategies in line with national disaster risk reduction strategies\n",
    "\n",
    "As this column talks about proportion, we are expecting a value from 0 to 100 again. This means that we can check it using the same function as the previous columns (i.e., the [`describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) function), in order to recheck this using the returned minimum and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea1b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data['1.5.4 Proportion of local governments that adopt and implement local disaster risk reduction strategies in line with national disaster risk reduction strategies'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eefb7f",
   "metadata": {},
   "source": [
    "Since the maximum is 100 and the minimum is not less than 0, then we can conclude that there are no values that are outside of the accepted range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c8ad9",
   "metadata": {},
   "source": [
    "#### 3.4.1 Mortality rate attributed to cardiovascular disease, cancer, diabetes or chronic respiratory disease (Total data)\n",
    "As the next column talks about a rate again, the accepted value range is within 0 to 100. This is because of its formula wherein we divide the number of people who died attributed to the said diseases by the population. Since the number of deaths cannot be higher than the population, we cannot accept a value higher than 100.\n",
    "\n",
    "We can check for the minimum and maximum value through the use of the [`describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb61dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['3.4.1 Mortality rate attributed to cardiovascular disease, cancer, diabetes or chronic respiratory disease (Total data)'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42651890",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d06dca41",
   "metadata": {},
   "source": [
    "#### 3.7.1 Proportion of women of reproductive age (aged 15-49 years) who have their need for family planning satisfied [provided] with modern methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf69981",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['3.7.1 Proportion of women of reproductive age (aged 15-49 years) who have their need for family planning satisfied [provided] with modern methods'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06966237",
   "metadata": {},
   "source": [
    "#### 3.7.2 Adolescent birth rate aged 15-19 years per 1,000 women in that age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d30d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['3.7.2 Adolescent birth rate aged 15-19 years per 1,000 women in that age group'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf8474",
   "metadata": {},
   "source": [
    "#### 4.1.s1 Completion Rate of elementary and secondary students (Elementary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90b70c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['4.1.s1 Completion Rate of elementary and secondary students (Elementary)'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa32eda",
   "metadata": {},
   "source": [
    "#### 4.1.s1 Completion Rate of elementary and secondary students (Junior High School)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['4.1.s1 Completion Rate of elementary and secondary students (Junior High School)'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc0e65",
   "metadata": {},
   "source": [
    "#### 4.1.s1 Completion Rate of elementary and secondary students (Senior High School)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['4.1.s1 Completion Rate of elementary and secondary students (Senior High School)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c658bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data[~combined_data['4.1.s1 Completion Rate of elementary and secondary students (Senior High School)'].isnull()]['Year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb9213f",
   "metadata": {},
   "source": [
    "#### 7.1.1 Proportion of population with access to electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24668c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['7.1.1 Proportion of population with access to electricity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c24a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data[combined_data['7.1.1 Proportion of population with access to electricity'] > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.loc[combined_data['7.1.1 Proportion of population with access to electricity'] > 100, '7.1.1 Proportion of population with access to electricity'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe48bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data[combined_data['7.1.1 Proportion of population with access to electricity'] > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b98ec",
   "metadata": {},
   "source": [
    "#### 10.1.1.1 Growth rates of household expenditure or income per capita among the bottom 40 per cent of the population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f4c904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f8ea48f",
   "metadata": {},
   "source": [
    "#### 10.1.1.2 Growth rates of household expenditure or income per capita among the Total Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb86116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "434b32a5",
   "metadata": {},
   "source": [
    "#### 14.5.1.1 Coverage of protected areas in relation to marine areas, Universe (in million hectares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825cec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c6939ad",
   "metadata": {},
   "source": [
    "#### 14.5.1.2 Coverage of protected areas in relation to marine areas, NIPAS and Locally managed MPAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ed5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "449f2394",
   "metadata": {},
   "source": [
    "#### 16.1.1 Number of victims of intentional homicide (per 100,000 population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7cc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dcd7b39",
   "metadata": {},
   "source": [
    "#### 16.1.s1 Number of murder cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d1020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f1c57da",
   "metadata": {},
   "source": [
    "#### Changes in Inventories, by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fccd428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e548eb9d",
   "metadata": {},
   "source": [
    "#### Current Health Expenditure by Region, Growth Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cfc513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75f5879b",
   "metadata": {},
   "source": [
    "#### Current Health Expenditure by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b81bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d52d041",
   "metadata": {},
   "source": [
    "#### Government Final Consumption Expenditure, by Region, Growth Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a843c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "491ccf6c",
   "metadata": {},
   "source": [
    "#### Government Final Consumption Expenditure, by Region, Percent Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7d6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8456911",
   "metadata": {},
   "source": [
    "#### Gross Capital Formation, by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc421cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a3f67bb",
   "metadata": {},
   "source": [
    "#### Gross Regional Domestic Product, by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24b55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d69b4ce3",
   "metadata": {},
   "source": [
    "#### Population, by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48725fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "608ef6c0",
   "metadata": {},
   "source": [
    "#### Primary Drop-out rates by Region, Sex and Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f9059d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73a563d3",
   "metadata": {},
   "source": [
    "#### Quarterly Producer Price Index for Agriculture (First Quarter 2018 to Third Quarter 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5ae21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c3a85c0-6464-4201-8f7b-2fd795a0a756",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "With the combined dataset, there is a substantial amount of raw data to process and analyze. Before performing any statistical analysis, it is good practice to do exploratory data analysis to observe patterns and detect any outliers in the dataset. With this, we can properly identify particular relationships between specific variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962baab8",
   "metadata": {},
   "source": [
    "### Per year, what region has the lowest proportion value of the population living below the national poverty line?\n",
    "To answer this questio, we would be utilizing three columns from the combined DataFrame: (1) `Geolocation`, (2) `Year`. (3) `1.2.1. Proportion of population living below the national poverty line`. However, since we aim to get the lowest proportion value per year, we would first need to group the rows, using the [`groupby`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) function. \n",
    "\n",
    "Once the rows has been grouped, per group, we would be getting the [`min`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.min.html)imum value for the `1.2.1. Proportion of population living below the national poverty line` column. Then, as we know from the cleaning that there are years without values for this column, we would only be getting the years that has not **NaN** as its minimum value, using the [`notna`](https://pandas.pydata.org/docs/reference/api/pandas.Series.notna.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad0589",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped_by_year = combined_data.groupby(['Year'])['1.2.1. Proportion of population living below the national poverty line'].min()\n",
    "grouped_by_year = grouped_by_year[grouped_by_year.notna()]\n",
    "grouped_by_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb4435",
   "metadata": {},
   "source": [
    "Then, once we got the minimum values for this column, we can use this to get the rows that has this minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data[combined_data ['1.2.1. Proportion of population living below the national poverty line'].isin(grouped_by_year.values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba86542",
   "metadata": {},
   "source": [
    "From the DataFrame, we can see that the **National Capital Region** has the lowest proportion value of the population living below the national poverty line for both of the years, and that it even decreased in the year 2018. \n",
    "\n",
    "Let us cross-check this using bar graph, wherein we would be able to see the proportion value of the population living below the national poverty line per region clearly. \n",
    "\n",
    "To do this, let us first get all the rows for **2015** and **2018**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc208bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2015 = combined_data[combined_data['Year'] == 2015]\n",
    "data_2018 = combined_data[combined_data['Year'] == 2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf044c7",
   "metadata": {},
   "source": [
    "Let us plot the data from 2015 into a bar graph using the [`plot`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1866ef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax1 = data_2015['1.2.1. Proportion of population living below the national poverty line'].plot(figsize=(8, 6), kind='bar', width=0.5)\n",
    "\n",
    "ax1.set_xticklabels(data_2015['Geolocation'], rotation=90)\n",
    "\n",
    "ax1.set_title('1.2.1. Proportion of population living below the national poverty line by Geolocation')\n",
    "ax1.set_ylabel('Proportion');\n",
    "ax1.set_xlabel('Geolocation');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad984a29",
   "metadata": {},
   "source": [
    "#### Figure 1. Proportion of population living below the national poverty line by Geolocation (2015)\n",
    "From the above figure, we can see that the bar of the **National Capital Region** is lower than the other regions. It has a proportion of lower than 10%, compared to the other graphs that look near 10% or higher. \n",
    "\n",
    "Using the same [`plot`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) function, let us also plot the data from 2018 into a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f823d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = data_2018['1.2.1. Proportion of population living below the national poverty line'].plot(figsize=(8, 6), kind='bar', width=0.5)\n",
    "\n",
    "ax2.set_xticklabels(data_2018['Geolocation'], rotation=90)\n",
    "\n",
    "ax2.set_title('1.2.1. Proportion of population living below the national poverty line by Geolocation')\n",
    "ax2.set_ylabel('Proportion');\n",
    "ax2.set_xlabel('Geolocation');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be567ff",
   "metadata": {},
   "source": [
    "#### Figure 2. Proportion of population living below the national poverty line by Geolocation (2018)\n",
    "For the `Year` 2018, we can see that the **National Capital Region** still has the shortest bar. Compared to the 2016 bar of  the region, this one is shorter.\n",
    "\n",
    "From these two bar graphs, we can conclude that the **National Capital Region** has the lowest proportion value of the population living below the national poverty line for the years available in the dataset (i.e., 2016 and 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf524de",
   "metadata": {},
   "source": [
    "### What education level (Junior or Senior High School) has a higher rate per region (2016 - 2018)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6903a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5211bac",
   "metadata": {},
   "source": [
    "### What year has the most adolescent birth rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbfcc06-2999-49e2-a540-0a3418816aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b05cf8ac",
   "metadata": {},
   "source": [
    "## Conversion of DataFrame to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223beae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f49118f4",
   "metadata": {},
   "source": [
    "## References\n",
    "https://www.un.org/esa/sustdev/natlinfo/indicators/methodology_sheets/education/net_enrolment.pdf\n",
    "https://psa.gov.ph/content/net-enrolment-ratio-ner-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eaa556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
